{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball Playoffs Predictive Model\n",
    "\n",
    "## Data Understanding/Preparation (??)\n",
    "\n",
    "After understanding the data meaning we are ready to start processing it. Our first step in this phase is the creation of a SQLite database in order to speed up the data accesses and also to facilitate the joining of data from the various tables.\n",
    "\n",
    "To fulfil the data needs we defined this schema:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE awards_players (\n",
    "    playerID TEXT,\n",
    "    award    TEXT,\n",
    "    year     INTEGER,\n",
    "    lgID     TEXT,\n",
    "    CONSTRAINT fk_playerID FOREIGN KEY (\n",
    "        playerID\n",
    "    )\n",
    "    REFERENCES players (bioID) \n",
    ");\n",
    "\n",
    "CREATE TABLE coaches (\n",
    "    coachID     TEXT,\n",
    "    year        INTEGER,\n",
    "    tmID        TEXT,\n",
    "    lgID        TEXT,\n",
    "    stint       INTEGER,\n",
    "    won         INTEGER,\n",
    "    lost        INTEGER,\n",
    "    post_wins   INTEGER,\n",
    "    post_losses INTEGER,\n",
    "    PRIMARY KEY (\n",
    "        coachID,\n",
    "        year,\n",
    "        tmID,\n",
    "        stint\n",
    "    ),\n",
    "    CONSTRAINT fk_year_tmID FOREIGN KEY (\n",
    "        year,\n",
    "        tmID\n",
    "    )\n",
    "    REFERENCES teams (year,\n",
    "    tmID) \n",
    ");\n",
    "\n",
    "CREATE TABLE players (\n",
    "    bioID        TEXT    PRIMARY KEY,\n",
    "    pos          TEXT,\n",
    "    firstseason  INTEGER,\n",
    "    lastseason   INTEGER,\n",
    "    height       REAL,\n",
    "    weight       INTEGER,\n",
    "    college      TEXT,\n",
    "    collegeOther TEXT,\n",
    "    birthDate    TEXT,\n",
    "    deathDate    TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE players_teams (\n",
    "    playerID           TEXT,\n",
    "    year               INTEGER,\n",
    "    stint              INTEGER,\n",
    "    tmID               TEXT,\n",
    "    lgID               TEXT,\n",
    "    GP                 INTEGER,\n",
    "    GS                 INTEGER,\n",
    "    minutes            INTEGER,\n",
    "    points             INTEGER,\n",
    "    oRebounds          INTEGER,\n",
    "    dRebounds          INTEGER,\n",
    "    rebounds           INTEGER,\n",
    "    assists            INTEGER,\n",
    "    steals             INTEGER,\n",
    "    blocks             INTEGER,\n",
    "    turnovers          INTEGER,\n",
    "    PF                 INTEGER,\n",
    "    fgAttempted        INTEGER,\n",
    "    fgMade             INTEGER,\n",
    "    ftAttempted        INTEGER,\n",
    "    ftMade             INTEGER,\n",
    "    threeAttempted     INTEGER,\n",
    "    threeMade          INTEGER,\n",
    "    dq                 INTEGER,\n",
    "    PostGP             INTEGER,\n",
    "    PostGS             INTEGER,\n",
    "    PostMinutes        INTEGER,\n",
    "    PostPoints         INTEGER,\n",
    "    PostoRebounds      INTEGER,\n",
    "    PostdRebounds      INTEGER,\n",
    "    PostRebounds       INTEGER,\n",
    "    PostAssists        INTEGER,\n",
    "    PostSteals         INTEGER,\n",
    "    PostBlocks         INTEGER,\n",
    "    PostTurnovers      INTEGER,\n",
    "    PostPF             INTEGER,\n",
    "    PostfgAttempted    INTEGER,\n",
    "    PostfgMade         INTEGER,\n",
    "    PostftAttempted    INTEGER,\n",
    "    PostftMade         INTEGER,\n",
    "    PostthreeAttempted INTEGER,\n",
    "    PostthreeMade      INTEGER,\n",
    "    PostDQ             INTEGER,\n",
    "    CONSTRAINT fk_playerID FOREIGN KEY (\n",
    "        playerID\n",
    "    )\n",
    "    REFERENCES players (bioID),\n",
    "    CONSTRAINT fk_year_tmID FOREIGN KEY (\n",
    "        year,\n",
    "        tmID\n",
    "    )\n",
    "    REFERENCES teams (year,\n",
    "    tmID) \n",
    ");\n",
    "\n",
    "CREATE TABLE series_post (\n",
    "    year       INTEGER,\n",
    "    round      TEXT,\n",
    "    series     TEXT,\n",
    "    tmIDWinner TEXT,\n",
    "    lgIDWinner TEXT,\n",
    "    tmIDLoser  TEXT,\n",
    "    lgIDLoser  TEXT,\n",
    "    W          INTEGER,\n",
    "    L          INTEGER,\n",
    "    CONSTRAINT fk_year_tmIDWinner FOREIGN KEY (\n",
    "        year,\n",
    "        tmIDWinner\n",
    "    )\n",
    "    REFERENCES teams (year,\n",
    "    tmID),\n",
    "    CONSTRAINT fk_year_tmIDLoser FOREIGN KEY (\n",
    "        year,\n",
    "        tmIDLoser\n",
    "    )\n",
    "    REFERENCES teams (year,\n",
    "    tmID) \n",
    ");\n",
    "\n",
    "CREATE TABLE teams (\n",
    "    year       INTEGER,\n",
    "    lgID       TEXT,\n",
    "    tmID       TEXT,\n",
    "    franchID   TEXT,\n",
    "    confID     TEXT,\n",
    "    divID      TEXT,\n",
    "    rank       INTEGER,\n",
    "    playoff    TEXT,\n",
    "    seeded     INTEGER,\n",
    "    firstRound TEXT,\n",
    "    semis      TEXT,\n",
    "    finals     TEXT,\n",
    "    name       TEXT,\n",
    "    o_fgm      INTEGER,\n",
    "    o_fga      INTEGER,\n",
    "    o_ftm      INTEGER,\n",
    "    o_fta      INTEGER,\n",
    "    o_3pm      INTEGER,\n",
    "    o_3pa      INTEGER,\n",
    "    o_oreb     INTEGER,\n",
    "    o_dreb     INTEGER,\n",
    "    o_reb      INTEGER,\n",
    "    o_asts     INTEGER,\n",
    "    o_pf       INTEGER,\n",
    "    o_stl      INTEGER,\n",
    "    o_to       INTEGER,\n",
    "    o_blk      INTEGER,\n",
    "    o_pts      INTEGER,\n",
    "    d_fgm      INTEGER,\n",
    "    d_fga      INTEGER,\n",
    "    d_ftm      INTEGER,\n",
    "    d_fta      INTEGER,\n",
    "    d_3pm      INTEGER,\n",
    "    d_3pa      INTEGER,\n",
    "    d_oreb     INTEGER,\n",
    "    d_dreb     INTEGER,\n",
    "    d_reb      INTEGER,\n",
    "    d_asts     INTEGER,\n",
    "    d_pf       INTEGER,\n",
    "    d_stl      INTEGER,\n",
    "    d_to       INTEGER,\n",
    "    d_blk      INTEGER,\n",
    "    d_pts      INTEGER,\n",
    "    tmORB      INTEGER,\n",
    "    tmDRB      INTEGER,\n",
    "    tmTRB      INTEGER,\n",
    "    opptmORB   INTEGER,\n",
    "    opptmDRB   INTEGER,\n",
    "    opptmTRB   INTEGER,\n",
    "    won        INTEGER,\n",
    "    lost       INTEGER,\n",
    "    GP         INTEGER,\n",
    "    homeW      INTEGER,\n",
    "    homeL      INTEGER,\n",
    "    awayW      INTEGER,\n",
    "    awayL      INTEGER,\n",
    "    confW      INTEGER,\n",
    "    confL      INTEGER,\n",
    "    min        INTEGER,\n",
    "    attend     INTEGER,\n",
    "    arena      TEXT,\n",
    "    PRIMARY KEY (\n",
    "        year,\n",
    "        tmID\n",
    "    )\n",
    ");\n",
    "```\n",
    "\n",
    "Then we populated the database with the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import sqlite3\n",
    "import subprocess\n",
    "\n",
    "# Create a shell script\n",
    "# with open('myscript.sh', 'w') as f:\n",
    "#     f.write('cat ./database/final.sql | sqlite3 ./database/bdfinal.sql')\n",
    "\n",
    "# Execute the script in WSL\n",
    "# subprocess.run([\"wsl\", \"./myscript.sh\"], check=True,shell=True)\n",
    "\n",
    "connection = sqlite3.connect(\"./database/bdfinal.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Before we train models on this data we need to assess its quality. One important step is finding missing values.\n",
    "\n",
    "We will look for missing values in all the created tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM coaches;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "## ---//---\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "col_names = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ## iterate through columns\n",
    "    found = False\n",
    "    if row[\"pos\"] == \"\":\n",
    "        found = True\n",
    "    elif row[\"height\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"weight\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"birthDate\"] == \"\" or row[\"birthDate\"] == \"0000-00-00\":\n",
    "        found = True\n",
    "    elif row[\"college\"] == \"\":\n",
    "        found = True\n",
    "\n",
    "    if(found):\n",
    "        print(row[\"bioID\"])\n",
    "    \n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players_teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM series_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player\n",
    "\n",
    "- After a quick glance at the data, it's easy to see that there's a certain amount of players that have many important missing/null values (college, height and weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select bioID from players where weight = 0 or height = 0 or college = '' or pos = '';\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets check if any of these players with a null position are actually coaches, since the awards_players tables has a coach award and references the players table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select count(*) from players where pos = \"\";\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '';\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n",
    "\n",
    "# select count(*) from players where pos = \"\" and bioID in (select coachID from coaches);\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '' and bioID in (select coachID from coaches);\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 52 out of the 78 players without position are coaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these 208 players, it's important to see which actually were a part of a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_missing_values_players = pd.read_sql(\"select distinct(bioID), weight, height, pos from players where (weight = 0 or height = 0) and pos <> ''\", connection)\n",
    "print(active_missing_values_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regarding these 83 players, if a player doesn't have their position missing, we decided to replace their missing weight and/or height values with the average value of the players of their same position. \n",
    "\n",
    "    - Obtain the average weight and height for each player position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query para cada valor\n",
    "avg_pos_weights = pd.read_sql(\"select pos, avg(weight) from players where weight <> 0 group by pos;\", connection)\n",
    "print(avg_pos_weights)\n",
    "avg_pos_heights = pd.read_sql(\"select pos, avg(height) from players where height <> 0 group by pos;\", connection)\n",
    "print(avg_pos_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Store the values in two dictionaries, where the key values are the players' positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to a dictionary where the key are the positions and the values are the avg weights\n",
    "avg_weights = {}\n",
    "\n",
    "for index, row in avg_pos_weights.iterrows():\n",
    "    avg_weights[row[\"pos\"]] = row[\"avg(weight)\"]\n",
    "    \n",
    "print(avg_weights)\n",
    "\n",
    "avg_heights = {}\n",
    "\n",
    "for index, row in avg_pos_heights.iterrows():\n",
    "    avg_heights[row[\"pos\"]] = row[\"avg(height)\"]\n",
    "    \n",
    "print(avg_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in active_missing_values_players.iterrows():\n",
    "    player = pd.read_sql(\"select * from players where bioID = '\" + row[\"bioID\"] + \"';\", connection)\n",
    "    \n",
    "    pos = player[\"pos\"].values[0]\n",
    "    if pos == '':\n",
    "        continue\n",
    "    \n",
    "    if(player[\"weight\"] != 0 and player[\"height\"] != 0):\n",
    "        # print(\"Player already has values\")\n",
    "        # print(player)\n",
    "        continue\n",
    "    \n",
    "    print(player)\n",
    "    \n",
    "    print(\"\\n===\\n\")\n",
    "\n",
    "    ## get average values for the player's position pos\n",
    "    if(player[\"weight\"].values[0] == 0):\n",
    "        weight = avg_weights[pos]\n",
    "    else:\n",
    "        weight = player[\"weight\"].values[0]\n",
    "    if (player[\"height\"].values[0] == 0):\n",
    "        height = avg_heights[pos]\n",
    "    else:\n",
    "        height = player[\"height\"].values[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    ## get row index\n",
    "    pos = row.index[0]\n",
    "    \n",
    "    # update player's height and weight\n",
    "    print(\"UPDATE players SET height = '\" + str(height) + \"', weight = '\" + str(weight) + \"' WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    \n",
    "    # update player's height and weight\n",
    "    connection.execute(\"UPDATE players SET height = \" + str(height) + \", weight = \" + str(weight) + \" WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    connection.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check if there are any outliers in the data\n",
    "\n",
    "## Player\n",
    "\n",
    "In this table we will be looking for outliers in weight, height and birth dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player weight distribution\n",
    "df = pd.read_sql(\"SELECT weight FROM players;\",connection)\n",
    "df = df[df.weight != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player weight distribution\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"weight\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player height distribution\n",
    "df = pd.read_sql(\"SELECT height FROM players;\",connection)\n",
    "df = df[df.height != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player height distribution\")\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"height\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there is one player with a height of 9.0. We can fix this via mean imputation, which means that her height will be replaced by the average height of the players that play in the same position as her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select player with height < 20\n",
    "df = pd.read_sql(\"SELECT * FROM players WHERE height < 20 and height > 0;\",connection)\n",
    "\n",
    "# get the average height for the player's position\n",
    "average_height = avg_heights[df[\"pos\"].values[0]]\n",
    "\n",
    "# update player's height\n",
    "connection.execute(\"UPDATE players SET height = \" + str(average_height) + \" WHERE bioID = '\" + df[\"bioID\"].values[0] + \"';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player birth year distribution\n",
    "df = pd.read_sql(\"SELECT birthDate FROM players;\",connection)\n",
    "df = df[df.birthDate != \"0000-00-00\"]\n",
    "\n",
    "#convert birthdate to year\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player birth year distribution\")\n",
    "plt.xlabel(\"Birth year\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"birthDate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent data\n",
    "\n",
    "### Player Awards\n",
    "\n",
    "- Check if there's any award, that should be given to one player, is given to two or more players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\" select count(playerID), award, year from awards_players group by award, year;\", connection)\n",
    "\n",
    "# print rows \n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we noticed that there's an award missing part of its title. Therefore, we'll have to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute(\"UPDATE awards_players SET award = 'Kim Perrot Sportsmanship Award' WHERE award = 'Kim Perrot Sportsmanship';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post\n",
    "\n",
    "- Check if, in any year, no more than 8 teams passed to the playoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select count(tmID) as num, year from teams_post group by year having num > 8;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if, in any year, only one team won the playoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, finals from teams where finals = 'W' order by year;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "- Check if the sum of games won and lost by a player is equal to the total games played by a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, won, lost, GP, (won + lost) as Games from teams where Games <> GP;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the sum of rebounds made by a team is equal to the sum of offensive rebounds and defensive rebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_sql(\"select year, tmID, o_oreb, o_dreb, o_reb, (o_oreb + o_dreb) as rebounds from teams where o_reb <> rebounds;\", connection)\n",
    "print(dataFrame)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataFrame = pd.read_sql(\"select year, tmID, d_oreb, d_dreb, d_reb, (d_oreb + d_dreb) as rebounds from teams where d_reb <> rebounds;\", connection)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the stats (field goals, 3 pointers, free throws, etc.) attempted are in a bigger quantity than the stats made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_fgm > o_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_ftm > o_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_3pm > o_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_fgm > d_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_ftm > d_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_3pm > d_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing irrelevant Columns\n",
    "\n",
    "Columns that are entirely composed by missing values can be removed. The league id column can also be dropped since the entire dataset belongs to the same league."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmTRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmTRB;\")\n",
    "\n",
    "# remove franchID and lgID from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN franchID;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove firstSeason and lastSeason from players using a query\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN firstSeason;\")\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN lastSeason;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgIDWinner, lgIDLoser and series from series_post using a query\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDWinner;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDLoser;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN series;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from teams_post using a query\n",
    "connection.execute(\"ALTER TABLE teams_post DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from awards_players using a query\n",
    "connection.execute(\"ALTER TABLE awards_players DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from players_teams using a query\n",
    "connection.execute(\"ALTER TABLE players_teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from coaches using a query\n",
    "connection.execute(\"ALTER TABLE coaches DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying models\n",
    "\n",
    "Considering the data context we decided to develop a more complex model that considers teams as a set of players and coach.\n",
    "\n",
    "Our goal is to make the final prediction based in the expected players performance according to their recorded statistics in the previous years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "# connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stint', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_10 = df[df[\"year\"] == 10][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df['birthDate'])\n",
    "\n",
    "# # get all rows from df where year = 10\n",
    "test_data = df.loc[df[\"year\"] == 10]\n",
    "\n",
    "# # get all rows from df where year <> 10\n",
    "train_data = df.loc[df[\"year\"] < 10]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100, 100, 100), \n",
    "    max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001))\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# test model\n",
    "predictions = model.predict(test_inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all predictions to integers\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        predictions[i][j] = int(round(predictions[i][j]))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    # print(\"player: \", i)\n",
    "    print(\"player: \", player_ids_10[i])\n",
    "    for j in range(len(predictions[i])):\n",
    "        print(labels[j], \":  predicted: \", predictions[i][j], \" actual: \", test_labels[i][j])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get team ids from the dataframe from all years \n",
    "team_ids = set(df[\"tmID\"].values)\n",
    "\n",
    "# create a dictionary where the keys are the team ids and the values are the avg stats for the team\n",
    "avg_team_stats = {}\n",
    "\n",
    "for team_id in team_ids:\n",
    "    avg_team_stats[team_id] = [0] * len(labels)\n",
    "    \n",
    "print(test_data.columns)\n",
    "\n",
    "# iterate through the team ids\n",
    "for team_id in team_ids:\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    " \n",
    "        if(test_data[\"tmID_\" + team_id].values[i] == 1):\n",
    "            avg_team_stats[team_id] = [x + y for x, y in zip(avg_team_stats[team_id], predictions[i])]\n",
    "            \n",
    "        \n",
    "# remove tmID from the dictionary that contain all the avg stats for the teams at 0\n",
    "for team_id in team_ids:\n",
    "    if(avg_team_stats[team_id] == [0] * len(labels)):\n",
    "        del avg_team_stats[team_id]\n",
    "        \n",
    "        \n",
    "print(avg_team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from teams where year <> 11;\"\n",
    "\n",
    "team_stats = pd.read_sql(query, connection)\n",
    "\n",
    "columns = ['tmID', 'year','o_pts', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_stl',\n",
    "       'o_blk', 'o_to', 'o_pf', 'o_fga', 'o_fgm', 'o_fta',\n",
    "       'o_ftm', 'o_3pa', 'o_3pm', 'confID', 'playoff']\n",
    "\n",
    "team_stats = team_stats[columns]\n",
    "\n",
    "query = \"select tmID from teams where year = 10;\"\n",
    "team_ids = pd.read_sql(query, connection)\n",
    "team_ids = team_ids[\"tmID\"].values\n",
    "\n",
    "team_stats = team_stats.drop(columns=[\"tmID\"])\n",
    "\n",
<<<<<<< HEAD
    "categorical_columns = ['confID']\n",
    "\n",
    "for col in categorical_columns:\n",
    "       team_stats[col] = team_stats[col].astype('category')\n",
    "       \n",
    "team_stats = pd.get_dummies(team_stats, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['playoff']\n",
    "inputs = []\n",
    "\n",
    "for col in team_stats.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "train_data = team_stats.loc[team_stats[\"year\"] != 10]\n",
    "\n",
    "print(train_data.columns)\n",
    "\n",
    "test_data = team_stats.loc[team_stats[\"year\"] == 10]\n",
    "\n",
    "# drop year column from train and test data\n",
    "train_data = train_data.drop(columns=[\"year\"])\n",
    "test_data = test_data.drop(columns=[\"year\"])\n",
    "\n",
    "# drop year from inputs\n",
    "inputs.remove(\"year\")\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs= []\n",
    "\n",
    "for team_id in avg_team_stats:\n",
    "    # append confID to the list\n",
    "    \n",
    "    query = \"select confID from teams where tmID = '\" + team_id + \"';\"\n",
    "    confID = pd.read_sql(query, connection)\n",
    "    confID = confID[\"confID\"].values[0]\n",
    "    \n",
    "    avg_team_stats[team_id].append(confID == 'EA')\n",
    "    avg_team_stats[team_id].append(confID == 'WE')\n",
    "    \n",
    "    test_inputs.append(avg_team_stats[team_id])\n",
    "    \n",
    "    \n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# print(train_inputs)\n",
    "# print(train_labels)\n",
    "\n",
    "print(test_inputs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "\n",
    "print(train_inputs)\n",
    "print(test_inputs)\n",
    "\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"select tmID from teams where year = 10;\"\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, alpha=0.0001, solver='adam', verbose=0, random_state=21, tol=0.000000001)\n",
    "mlp.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = mlp.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = mlp.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "clf.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = clf.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = clf.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=24)\n",
    "knn.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = knn.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = knn.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(train_inputs,train_labels)\n",
    "\n",
    "#print decision tree\n",
    "tree.plot_tree(dt)\n",
    "\n",
    "y_pred = dt.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = dt.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "        \n",
    "        team_id = team_ids[i]\n",
    "        \n",
    "        if team_id in west_teams[\"tmID\"].values:\n",
    "            probs_west.append((team_id, probs[i][1]))\n",
    "            \n",
    "        if team_id in east_teams[\"tmID\"].values:\n",
    "            probs_east.append((team_id, probs[i][1]))\n",
    "        \n",
    "        # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "        \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season 11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this part is adding the season 11 data to the database"
=======
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))"
>>>>>>> d8b54913b654385e42561d2b0a59428d9bded54d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/coaches.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO coaches (coachID, year, tmID, stint) VALUES ('\" + row[0] + \"', '\" + str(row[1]) + \"', '\" + row[2] +  \"', '\" + str(row[4]) + \"');\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/teams.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO teams (year, tmID, confID, name, arena) VALUES ('\" + str(row[0]) + \"', '\" + row[2] + \"', '\" + row[4] +  \"', '\" + row[5] + \"', '\" + row[6] + \"');\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/players_teams.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO players_teams (playerID, year, stINTEGER, tmID) VALUES ('\" + row[0] + \"', '\" + str(row[1]) + \"', '\" + str(row[2]) +  \"', '\" + row[3] + \"');\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> d8b54913b654385e42561d2b0a59428d9bded54d
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "# connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stint', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_11 = df[df[\"year\"] == 11][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df)\n",
    "# # get all rows from df where year = 11\n",
    "test_data = df.loc[df[\"year\"] == 11]\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "# # get all rows from df where year <> 11\n",
    "train_data = df.loc[df[\"year\"] != 11]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "# print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
