{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import sqlite3\n",
    "import subprocess\n",
    "\n",
    "# Create a shell script\n",
    "# with open('myscript.sh', 'w') as f:\n",
    "#     f.write('cat ./database/final.sql | sqlite3 ./database/bdfinal.sql')\n",
    "\n",
    "# Execute the script in WSL\n",
    "# subprocess.run([\"wsl\", \"./myscript.sh\"], check=True,shell=True)\n",
    "\n",
    "connection = sqlite3.connect(\"./database/bdfinal.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM coaches;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "## ---//---\n",
    "\n",
    "## get rows where pos = \"\"\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "## iterate through rows\n",
    "\n",
    "col_names = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ## iterate through columns\n",
    "    found = False\n",
    "    if row[\"pos\"] == \"\":\n",
    "        found = True\n",
    "    elif row[\"height\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"weight\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"birthDate\"] == \"\" or row[\"birthDate\"] == \"0000-00-00\":\n",
    "        found = True\n",
    "    elif row[\"college\"] == \"\":\n",
    "        found = True\n",
    "\n",
    "    if(found):\n",
    "        print(row[\"bioID\"])\n",
    "    \n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players_teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM series_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player\n",
    "\n",
    "- After a quick glance at the data, it's easy to see that there's a certain amount of players that have many important missing/null values (college, height and weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select bioID from players where weight = 0 or height = 0 or college = '' or pos = '';\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets check if any of these players with a null position are actually coaches, since the awards_players tables has a coach award and references the players table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select count(*) from players where pos = \"\";\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '';\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n",
    "\n",
    "# select count(*) from players where pos = \"\" and bioID in (select coachID from coaches);\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '' and bioID in (select coachID from coaches);\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 52 out of the 78 players without position are coaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these 208 players, it's important to see which actually were a part of a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_missing_values_players = pd.read_sql(\"select distinct(bioID), weight, height, pos from players where (weight = 0 or height = 0) and pos <> ''\", connection)\n",
    "print(active_missing_values_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regarding these 83 players, if a player doesn't have their position missing, we decided to replace their missing weight and/or height values with the average value of the players of their same position. \n",
    "\n",
    "    - Obtain the average weight and height for each player position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query para cada valor\n",
    "avg_pos_weights = pd.read_sql(\"select pos, avg(weight) from players where weight <> 0 group by pos;\", connection)\n",
    "print(avg_pos_weights)\n",
    "avg_pos_heights = pd.read_sql(\"select pos, avg(height) from players where height <> 0 group by pos;\", connection)\n",
    "print(avg_pos_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Store the values in two dictionaries, where the key values are the players' positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to a dictionary where the key are the positions and the values are the avg weights\n",
    "avg_weights = {}\n",
    "\n",
    "for index, row in avg_pos_weights.iterrows():\n",
    "    avg_weights[row[\"pos\"]] = row[\"avg(weight)\"]\n",
    "    \n",
    "print(avg_weights)\n",
    "\n",
    "avg_heights = {}\n",
    "\n",
    "for index, row in avg_pos_heights.iterrows():\n",
    "    avg_heights[row[\"pos\"]] = row[\"avg(height)\"]\n",
    "    \n",
    "print(avg_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in active_missing_values_players.iterrows():\n",
    "    player = pd.read_sql(\"select * from players where bioID = '\" + row[\"bioID\"] + \"';\", connection)\n",
    "    \n",
    "    pos = player[\"pos\"].values[0]\n",
    "    if pos == '':\n",
    "        continue\n",
    "    \n",
    "    if(player[\"weight\"] != 0 and player[\"height\"] != 0):\n",
    "        # print(\"Player already has values\")\n",
    "        # print(player)\n",
    "        continue\n",
    "    \n",
    "    print(player)\n",
    "    \n",
    "    print(\"\\n===\\n\")\n",
    "\n",
    "    ## get average values for the player's position pos\n",
    "    if(player[\"weight\"].values[0] == 0):\n",
    "        weight = avg_weights[pos]\n",
    "    else:\n",
    "        weight = player[\"weight\"].values[0]\n",
    "    if (player[\"height\"].values[0] == 0):\n",
    "        height = avg_heights[pos]\n",
    "    else:\n",
    "        height = player[\"height\"].values[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    ## get row index\n",
    "    pos = row.index[0]\n",
    "    \n",
    "    # update player's height and weight\n",
    "    print(\"UPDATE players SET height = '\" + str(height) + \"', weight = '\" + str(weight) + \"' WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    \n",
    "    # update player's height and weight\n",
    "    connection.execute(\"UPDATE players SET height = \" + str(height) + \", weight = \" + str(weight) + \" WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    connection.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check if there are any outliers in the data\n",
    "\n",
    "## Player\n",
    "\n",
    "In this table we will be looking for outliers in weight, height and birth dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player weight distribution\n",
    "df = pd.read_sql(\"SELECT weight FROM players;\",connection)\n",
    "df = df[df.weight != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player weight distribution\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"weight\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player height distribution\n",
    "df = pd.read_sql(\"SELECT height FROM players;\",connection)\n",
    "df = df[df.height != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player height distribution\")\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"height\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there is one player with a height of 9.0. We can fix this via mean imputation, which means that her height will be replaced by the average height of the players that play in the same position as her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select player with height < 20\n",
    "df = pd.read_sql(\"SELECT * FROM players WHERE height < 20 and height > 0;\",connection)\n",
    "\n",
    "# get the average height for the player's position\n",
    "average_height = avg_heights[df[\"pos\"].values[0]]\n",
    "\n",
    "# update player's height\n",
    "connection.execute(\"UPDATE players SET height = \" + str(average_height) + \" WHERE bioID = '\" + df[\"bioID\"].values[0] + \"';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player birth year distribution\n",
    "df = pd.read_sql(\"SELECT birthDate FROM players;\",connection)\n",
    "df = df[df.birthDate != \"0000-00-00\"]\n",
    "\n",
    "#convert birthdate to year\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player birth year distribution\")\n",
    "plt.xlabel(\"Birth year\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"birthDate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent data\n",
    "\n",
    "### Player Awards\n",
    "\n",
    "- Check if there's any award, that should be given to one player, is given to two or more players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\" select count(playerID), award, year from awards_players group by award, year;\", connection)\n",
    "\n",
    "# print rows \n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we noticed that there's an award missing part of its title. Therefore, we'll have to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute(\"UPDATE awards_players SET award = 'Kim Perrot Sportsmanship Award' WHERE award = 'Kim Perrot Sportsmanship';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post\n",
    "\n",
    "- Check if, in any year, no more than 8 teams passed to the playoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select count(tmID) as num, year from teams_post group by year having num > 8;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if, in any year, only one team won the playoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, finals from teams where finals = 'W' order by year;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "- Check if the sum of games won and lost by a player is equal to the total games played by a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, won, lost, GP, (won + lost) as Games from teams where Games <> GP;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the sum of rebounds made by a team is equal to the sum of offensive rebounds and defensive rebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_sql(\"select year, tmID, o_oreb, o_dreb, o_reb, (o_oreb + o_dreb) as rebounds from teams where o_reb <> rebounds;\", connection)\n",
    "print(dataFrame)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataFrame = pd.read_sql(\"select year, tmID, d_oreb, d_dreb, d_reb, (d_oreb + d_dreb) as rebounds from teams where d_reb <> rebounds;\", connection)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the stats (field goals, 3 pointers, free throws, etc.) attempted are in a bigger quantity than the stats made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_fgm > o_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_ftm > o_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_3pm > o_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_fgm > d_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_ftm > d_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_3pm > d_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmTRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmTRB;\")\n",
    "\n",
    "# remove franchID and lgID from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN franchID;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove firstSeason and lastSeason from players using a query\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN firstSeason;\")\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN lastSeason;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgIDWinner, lgIDLoser and series from series_post using a query\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDWinner;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDLoser;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN series;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from teams_post using a query\n",
    "connection.execute(\"ALTER TABLE teams_post DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from awards_players using a query\n",
    "connection.execute(\"ALTER TABLE awards_players DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from players_teams using a query\n",
    "connection.execute(\"ALTER TABLE players_teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from coaches using a query\n",
    "connection.execute(\"ALTER TABLE coaches DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dataframe without any column names, indices or data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "## obtain the team IDS and year\n",
    "team_ids = pd.read_sql(\"select tmID, year, confID from teams order by tmID\", connection)\n",
    "\n",
    "\n",
    "\n",
    "## iterate through each team and year\n",
    "for index, row in team_ids.iterrows():\n",
    "\n",
    "    ## obtain the team ID and year for each row\n",
    "    team_id = row['tmID']\n",
    "    year = row['year']\n",
    "    confID = row['confID']\n",
    "\n",
    "    # get all players from the team and year\n",
    "    query = \"select tmID, year, playerID from players_teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "\n",
    "    ## obtain the players for each team\n",
    "    team_players = pd.read_sql(query, connection)\n",
    "    \n",
    "    team_stats = {\"year\": year, \"points\": 0, \"oRebounds\": 0, \"dRebounds\": 0, \"rebounds\": 0,\n",
    "                  \"assists\": 0, \"steals\": 0, \"blocks\": 0, \"turnovers\": 0, \"PF\": 0, \"fgAttempted\": 0,\n",
    "                  \"fgMade\": 0, \"ftAttempted\": 0, \"ftMade\": 0, \"threeAttempted\": 0, \"threeMade\": 0,\n",
    "                  'weight': 0, 'height': 0, \"player_awards\": 0, \"confID\": confID, \"num_playoffs\": 0,\n",
    "                  \"num_semis\": 0, \"num_finals\": 0, \"coach_win_ratio\": 0, \"coach_stint\": 0,\n",
    "                  \"playoff\": 0}\n",
    "\n",
    "    # iterate through each player\n",
    "    for idx, r in team_players.iterrows():\n",
    "        ## obtain the player ID for each row\n",
    "        player_id = r['playerID']\n",
    "\n",
    "        # get the player's position\n",
    "        query = \"select pos from players where bioID = '\" + player_id + \"';\"\n",
    "        pos = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        # get number of years played\n",
    "        query = \"select count(distinct year) as num_years from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        num_years = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        if num_years == 0:\n",
    "            query = \"select avg(points) as points, avg(oRebounds) as oRebounds, avg(dRebounds) as dRebounds, avg(rebounds) as rebounds, avg(assists) as assists, \\\n",
    "                avg(steals) as steals, avg(blocks) as blocks, avg(turnovers) as turnovers, avg(PF) as PF, avg(fgAttempted) as fgAttempted, \\\n",
    "                avg(fgMade) as fgMade, avg(ftAttempted) as ftAttempted, avg(ftMade) as ftMade, avg(threeAttempted) as threeAttempted, avg(threeMade) as threeMade \\\n",
    "                from players_teams join players on players_teams.playerID = players.bioID where year = \" + str(year - 1) + \" \\\n",
    "                and pos = '\" + pos + \"' and playerID not in (select playerID from players_teams where year < \" + str(year - 1) + \");\"\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            ## add the player stats to the team stats\n",
    "            team_stats['points'] += (player_stats['points'].values[0] or 0 )\n",
    "            team_stats['oRebounds'] += (player_stats['oRebounds'].values[0] or 0 )\n",
    "            team_stats['dRebounds'] += (player_stats['dRebounds'].values[0] or 0 )\n",
    "            team_stats['rebounds'] += (player_stats['rebounds'].values[0] or 0 )\n",
    "            team_stats['assists'] += (player_stats['assists'].values[0] or 0 )\n",
    "            team_stats['steals'] += (player_stats['steals'].values[0] or 0 )\n",
    "            team_stats['blocks'] += (player_stats['blocks'].values[0] or 0 )\n",
    "            team_stats['turnovers'] += (player_stats['turnovers'].values[0] or 0 )\n",
    "            team_stats['PF'] += (player_stats['PF'].values[0] or 0 )\n",
    "            team_stats['fgAttempted'] += (player_stats['fgAttempted'].values[0] or 0 )\n",
    "            team_stats['fgMade'] += (player_stats['fgMade'].values[0] or 0 )\n",
    "            team_stats['ftAttempted'] += (player_stats['ftAttempted'].values[0] or 0 )\n",
    "            team_stats['ftMade'] += (player_stats['ftMade'].values[0] or 0 )\n",
    "            team_stats['threeAttempted'] += (player_stats['threeAttempted'].values[0] or 0 )\n",
    "            team_stats['threeMade'] += (player_stats['threeMade'].values[0] or 0 )\n",
    "            \n",
    "        else:\n",
    "\n",
    "            query = \"select year, points, oRebounds, dRebounds, rebounds, assists, \\\n",
    "                    steals, blocks, turnovers, PF, fgAttempted, \\\n",
    "                    fgMade, ftAttempted, ftMade, threeAttempted, sum(threeMade) as threeMade \\\n",
    "                    from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        \n",
    "            ## obtain the average stats for each team\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            points = oRebounds = dRebounds = rebounds = assists = steals = blocks = turnovers = PF = fgAttempted = fgMade = ftAttempted = ftMade = threeAttempted = threeMade = 0\n",
    "            sum_weight = 0\n",
    "            # iterate player stats and make weighted average for each year\n",
    "            for j, row2 in player_stats.iterrows():\n",
    "\n",
    "                iteration_year = row2['year']\n",
    "\n",
    "                weight = 1 / (year - iteration_year)\n",
    "\n",
    "                sum_weight += weight\n",
    "\n",
    "                points += row2['points'] * weight\n",
    "                oRebounds += row2['oRebounds'] * weight\n",
    "                dRebounds += row2['dRebounds'] * weight\n",
    "                rebounds += row2['rebounds'] * weight\n",
    "                assists += row2['assists'] * weight\n",
    "                steals += row2['steals'] * weight\n",
    "                blocks += row2['blocks'] * weight\n",
    "                turnovers += row2['turnovers'] * weight\n",
    "                PF += row2['PF'] * weight\n",
    "                fgAttempted += row2['fgAttempted'] * weight\n",
    "                fgMade += row2['fgMade'] * weight\n",
    "                ftAttempted += row2['ftAttempted'] * weight\n",
    "                ftMade += row2['ftMade'] * weight\n",
    "                threeAttempted += row2['threeAttempted'] * weight\n",
    "                threeMade += row2['threeMade'] * weight\n",
    "\n",
    "            # add the player stats to the team stats\n",
    "            team_stats['points'] += points / sum_weight\n",
    "            team_stats['oRebounds'] += oRebounds / sum_weight\n",
    "            team_stats['dRebounds'] += dRebounds / sum_weight\n",
    "            team_stats['rebounds'] += rebounds / sum_weight\n",
    "            team_stats['assists'] += assists / sum_weight\n",
    "            team_stats['steals'] += steals / sum_weight\n",
    "            team_stats['blocks'] += blocks / sum_weight\n",
    "            team_stats['turnovers'] += turnovers / sum_weight\n",
    "            team_stats['PF'] += PF / sum_weight\n",
    "            team_stats['fgAttempted'] += fgAttempted / sum_weight\n",
    "            team_stats['fgMade'] += fgMade / sum_weight\n",
    "            team_stats['ftAttempted'] += ftAttempted / sum_weight\n",
    "            team_stats['ftMade'] += ftMade / sum_weight\n",
    "            team_stats['threeAttempted'] += threeAttempted / sum_weight\n",
    "            team_stats['threeMade'] += threeMade / sum_weight\n",
    "        \n",
    "        num_years = max(num_years, 1)\n",
    "\n",
    "\n",
    "        # get each player num awards\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + player_id + \"' and ap.year < \" + str(year) + \";\"\n",
    "        num_awards_player = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['player_awards'] += num_awards_player\n",
    "\n",
    "        # get the number of times the player went to the playoffs\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and firstRound <> '';\"\n",
    "        num_playoffs = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_playoffs'] += num_playoffs\n",
    "\n",
    "        # get the number of times the player went to the semis\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and semis <> '';\"\n",
    "        num_semis = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_semis'] += num_semis\n",
    "\n",
    "        # get the number of times the player went to the finals\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and finals <> '';\"\n",
    "        num_finals = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_finals'] += num_finals\n",
    "    \n",
    "    \n",
    "    query = \"select avg(weight), avg(height) from players_teams join players on players.bioID = players_teams.playerID \\\n",
    "        where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "        \n",
    "    ## obtain the average weight and height for each team\n",
    "    team_weight_height = pd.read_sql(query, connection)\n",
    "    \n",
    "    ## add the average weight and height to the team stats\n",
    "    team_stats['weight'] = team_weight_height['avg(weight)'].values[0]\n",
    "    team_stats['height'] = team_weight_height['avg(height)'].values[0]\n",
    "\n",
    "    # get team coach\n",
    "    query = \"select coachID, stint from coaches where tmID = '\" + team_id + \"' and year = \"+ str(year) +\";\"\n",
    "    coach_id = pd.read_sql(query, connection).values[0][0]\n",
    "    stint = pd.read_sql(query, connection).values[0][1]\n",
    "    team_stats['coach_stint'] = stint\n",
    "    \n",
    "    \n",
    "    query = \"select count(award) as num_awards from coaches join awards_players \\\n",
    "              on coaches.coachID = awards_players.playerID and coaches.year = awards_players.year \\\n",
    "              where coachID = '\" + coach_id + \"' and coaches.year < \" + str(year) + \";\"\n",
    "    coach_awards = pd.read_sql(query, connection).values[0][0]\n",
    "    team_stats['coach_awards'] = coach_awards\n",
    "\n",
    "    # get average wins and losses from coach\n",
    "    query = \"select avg(won), avg(lost) from coaches where coachID = '\" + coach_id + \"' and year < \" + str(year) + \";\"\n",
    "    coach_wins_losses = pd.read_sql(query, connection)\n",
    "    num_matches_coach = (coach_wins_losses['avg(won)'].values[0] or 0) + (coach_wins_losses['avg(lost)'].values[0] or 0)\n",
    "    if num_matches_coach == 0:\n",
    "        team_stats['coach_win_ratio'] = 0\n",
    "    else:\n",
    "        team_stats['coach_win_ratio'] = (coach_wins_losses['avg(won)'].values[0] or 0) / num_matches_coach\n",
    "        \n",
    "        \n",
    "    query = \"select playoff from teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "    playoff = pd.read_sql(query, connection)\n",
    "    \n",
    "    # check if it's Y or N\n",
    "    if playoff['playoff'][0] == 'Y':\n",
    "        team_stats['playoff'] = 1\n",
    "    else:\n",
    "        team_stats['playoff'] = 0\n",
    "        \n",
    "    \n",
    "    # append the stats to the dataframe\n",
    "    df = df._append(team_stats, ignore_index=True)\n",
    "\n",
    "    print(\"Team id: \" + team_id)\n",
    "    print(team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"confID\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df= pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_year = 10\n",
    "\n",
    "# # get all rows from df where year = target year\n",
    "test_data = df.loc[df[\"year\"] == target_year]\n",
    "\n",
    "# # get all rows from df where year <> target_year and year <> 1\n",
    "train_data = df.loc[df[\"year\"] < target_year]\n",
    "train_data = train_data.loc[train_data[\"year\"] != 1]\n",
    "\n",
    "labels = ['playoff']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "print(train_inputs)\n",
    "print(train_labels)\n",
    "\n",
    "print(test_inputs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "logreg.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "feature_importance = abs(logreg.coef_[0])\n",
    "feature_importance = sorted(zip(inputs, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature importance:\")\n",
    "for i in range(len(feature_importance)):\n",
    "    print(f\"{feature_importance[i][0]}: {feature_importance[i][1]}\")\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(test_labels, logreg.predict(test_inputs))\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, logreg.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "query = \"select tmId, confID from teams where year = \" + str(target_year) + \";\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "team_ids = pd.read_sql(query, connection)\n",
    "team_ids = team_ids[\"tmID\"].values\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=2000, alpha=0.0001, solver='adam', verbose=0, random_state=21, tol=0.000000001)\n",
    "mlp.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = mlp.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = mlp.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "knn.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = knn.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = knn.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stINTEGER', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_10 = df[df[\"year\"] == 10][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1980\n",
      "1       1980\n",
      "2       1980\n",
      "3       1980\n",
      "4       1980\n",
      "        ... \n",
      "1871    1977\n",
      "1872    1976\n",
      "1873    1986\n",
      "1874    1980\n",
      "1875    1986\n",
      "Name: birthDate, Length: 1876, dtype: int32\n",
      "['height', 'weight', 'birthDate', 'year', 'stINTEGER', 'num_awards_player', 'bioID_abrossv01w', 'bioID_adamsjo01w', 'bioID_aguilel01w', 'bioID_ajavoma01w', 'bioID_aldrima01w', 'bioID_alhalta01w', 'bioID_allench01w', 'bioID_amachma01w', 'bioID_ambermo01w', 'bioID_anderam01w', 'bioID_anderch01w', 'bioID_anderjo01w', 'bioID_anderke01w', 'bioID_andrame01w', 'bioID_anosini01w', 'bioID_arcaija01w', 'bioID_askamma01w', 'bioID_atkinla01w', 'bioID_atunrmo01w', 'bioID_augusse01w', 'bioID_aycocan01w', 'bioID_azizle01w', 'bioID_azzije01w', 'bioID_badertr01w', 'bioID_bakersh01w', 'bioID_balesal01w', 'bioID_banchrh01w', 'bioID_baranel01w', 'bioID_barksla01w', 'bioID_barnead01w', 'bioID_barnequ01w', 'bioID_batkosu01w', 'bioID_batteja01w', 'bioID_battlas01w', 'bioID_bauerca01w', 'bioID_beardal01w', 'bioID_beckki01w', 'bioID_bennije01w', 'bioID_berezva01w', 'bioID_berthlu01w', 'bioID_beviltu01w', 'bioID_bibbyje01w', 'bioID_bibrzag01w', 'bioID_birdsu01w', 'bioID_bjorkte01w', 'bioID_blackch01w', 'bioID_blackde01w', 'bioID_blodgci01w', 'bioID_blueni01w', 'bioID_blueoc01w', 'bioID_bobbish01w', 'bioID_boddiwh01w', 'bioID_boltoru01w', 'bioID_bondla01w', 'bioID_bonfisu01w', 'bioID_bonnede01w', 'bioID_bowenli01w', 'bioID_boydca01w', 'bioID_braxtka01w', 'bioID_brazian01w', 'bioID_bristre01w', 'bioID_brogami01w', 'bioID_brondsa01w', 'bioID_brownco01w', 'bioID_browned01w', 'bioID_brownki01w', 'bioID_brownru01w', 'bioID_brumfma01w', 'bioID_brungje01w', 'bioID_brunsre01w', 'bioID_bullevi01w', 'bioID_burgean01w', 'bioID_burgeli01w', 'bioID_burraal01w', 'bioID_burseja01w', 'bioID_buttsta01w', 'bioID_byearla01w', 'bioID_campbed01w', 'bioID_campbmi01w', 'bioID_cantydo01w', 'bioID_careyja01w', 'bioID_carsoes01w', 'bioID_carteam01w', 'bioID_cashsw01w', 'bioID_cassija01w', 'bioID_castriz01w', 'bioID_catchta01w', 'bioID_caufila01w', 'bioID_chambco01w', 'bioID_chanequ01w', 'bioID_chapmji01w', 'bioID_choneka01w', 'bioID_chriska01w', 'bioID_chrissh01w', 'bioID_cironkr01w', 'bioID_clearmi01w', 'bioID_clinest01w', 'bioID_coggicl01w', 'bioID_cokermo01w', 'bioID_colemco01w', 'bioID_colemma01w', 'bioID_colleka01w', 'bioID_consuca01w', 'bioID_coopeca01w', 'bioID_coopecy01w', 'bioID_crawlsy01w', 'bioID_crockda01w', 'bioID_crockwi01w', 'bioID_cronika01w', 'bioID_cunnibe01w', 'bioID_cunnida01w', 'bioID_currimo01w', 'bioID_curryed01w', 'bioID_dalesst01w', 'bioID_daleygr01w', 'bioID_darlihe01w', 'bioID_davenje01w', 'bioID_davisbr01w', 'bioID_davisde01w', 'bioID_davista02w', 'bioID_deforan01w', 'bioID_derevje01w', 'bioID_desouer01w', 'bioID_dickeke01w', 'bioID_dillata01w', 'bioID_dixonta01w', 'bioID_donapbe01w', 'bioID_doronsh01w', 'bioID_dossaci01w', 'bioID_douglka01w', 'bioID_duffyme01w', 'bioID_dupreca01w', 'bioID_dydekma01w', 'bioID_edwarmi01w', 'bioID_edwarsi01w', 'bioID_edwarte01w', 'bioID_edwarto01w', 'bioID_elysh01w', 'bioID_enissh01w', 'bioID_erbsu01w', 'bioID_ervinla01w', 'bioID_fallotr01w', 'bioID_farriba01w', 'bioID_feastal01w', 'bioID_feifesu01w', 'bioID_ferdima01w', 'bioID_fernama01w', 'bioID_ferrama01w', 'bioID_figgsuk01w', 'bioID_firsool01w', 'bioID_floremi01w', 'bioID_flukety01w', 'bioID_folklkr01w', 'bioID_fordch01w', 'bioID_fordki01w', 'bioID_fordst01w', 'bioID_fowlesy01w', 'bioID_francde01w', 'bioID_frankaq01w', 'bioID_frazeme01w', 'bioID_fresest01w', 'bioID_frettla01w', 'bioID_friertr01w', 'bioID_frohlli01w', 'bioID_futreca01w', 'bioID_gaithka01w', 'bioID_garcibe01w', 'bioID_gardike01w', 'bioID_gardnan01w', 'bioID_garnean01w', 'bioID_gaydeco01w', 'bioID_gearlka01w', 'bioID_gibsoke01w', 'bioID_gilloje01w', 'bioID_gilmous01w', 'bioID_gisseka01w', 'bioID_givench01w', 'bioID_gomisem01w', 'bioID_goodsad01w', 'bioID_goringi01w', 'bioID_gortmsh01w', 'bioID_granter01w', 'bioID_grecomi01w', 'bioID_greenci01w', 'bioID_grginve01w', 'bioID_griffyo01w', 'bioID_groomla01w', 'bioID_grubigo01w', 'bioID_grudasa01w', 'bioID_hairska01w', 'bioID_hallam01w', 'bioID_hallvi01w', 'bioID_hammobe01w', 'bioID_hamzoro01w', 'bioID_hardili01w', 'bioID_harpela01w', 'bioID_harrido01w', 'bioID_harrili01w', 'bioID_harrokr01w', 'bioID_haydeva01w', 'bioID_haynikr01w', 'bioID_headde01w', 'bioID_hendene01w', 'bioID_hendetr01w', 'bioID_henniso01w', 'bioID_herriam01w', 'bioID_hibbeka01w', 'bioID_hicksje01w', 'bioID_hillec01w', 'bioID_hledeko01w', 'bioID_hodgedo01w', 'bioID_hodgero01w', 'bioID_hoffmeb01w', 'bioID_holdsch01w', 'bioID_hollake01w', 'bioID_holliqu01w', 'bioID_holmejo01w', 'bioID_holmese01w', 'bioID_holtam01w', 'bioID_hopeky01w', 'bioID_hornbal01w', 'bioID_houstch01w', 'bioID_humphta01w', 'bioID_irvinsa01w', 'bioID_ivanyda01w', 'bioID_iveyni01w', 'bioID_jacksde01w', 'bioID_jacksgw01w', 'bioID_jacksla01w', 'bioID_jacksta01w', 'bioID_jacksta02w', 'bioID_jacksti02w', 'bioID_jacobam01w', 'bioID_jamesta01w', 'bioID_januabr01w', 'bioID_jekaban01w', 'bioID_joensca01w', 'bioID_johnsad01w', 'bioID_johnsch01w', 'bioID_johnsja01w', 'bioID_johnsla01w', 'bioID_johnsni01w', 'bioID_johnspo01w', 'bioID_johnssh01w', 'bioID_johnste01w', 'bioID_johnsti01w', 'bioID_johnsvi01w', 'bioID_jonesas01w', 'bioID_jonesch01w', 'bioID_jonesja01w', 'bioID_jonesla01w', 'bioID_jonesme01w', 'bioID_kellycr01w', 'bioID_kingbsu01w', 'bioID_kingija01w', 'bioID_klimezu01w', 'bioID_koehnla01w', 'bioID_korstil01w', 'bioID_kostaan01w', 'bioID_kostita01w', 'bioID_kraayca01w', 'bioID_kubikni01w', 'bioID_lacyje01w', 'bioID_lacyve01w', 'bioID_lambesh01w', 'bioID_lambmo01w', 'bioID_langhcr01w', 'bioID_larkier01w', 'bioID_lassiam01w', 'bioID_lattaiv01w', 'bioID_lawsoed01w', 'bioID_lawsoka01w', 'bioID_lazicka01w', 'bioID_lehnish01w', 'bioID_lennobe01w', 'bioID_leslili01w', 'bioID_leuchye01w', 'bioID_levanni01w', 'bioID_lewista01w', 'bioID_lewisty01w', 'bioID_liebena01w', 'bioID_littlca01w', 'bioID_lloydan01w', 'bioID_lobore01w', 'bioID_lovelst01w', 'bioID_luckepa01w', 'bioID_luzhe01w', 'bioID_lyttlsa01w', 'bioID_mabikmw01w', 'bioID_macchla01w', 'bioID_machacl01w', 'bioID_mahonme01w', 'bioID_mahonsh01w', 'bioID_maigaha01w', 'bioID_malcona01w', 'bioID_malloso01w', 'bioID_maltsev01w', 'bioID_mannish01w', 'bioID_mannkr01w', 'bioID_mapprh01w', 'bioID_marcimi01w', 'bioID_martima01w', 'bioID_martinu01w', 'bioID_mascira01w', 'bioID_matteca01w', 'bioID_maxwemo01w', 'bioID_mazzake01w', 'bioID_mccaibr01w', 'bioID_mccaiti01w', 'bioID_mccanra01w', 'bioID_mccarja01w', 'bioID_mccarst01w', 'bioID_mcconsu01w', 'bioID_mccouan01w', 'bioID_mccrani01w', 'bioID_mccrini01w', 'bioID_mcculda01w', 'bioID_mcgheca01w', 'bioID_mckivte01w', 'bioID_mcwilta01w', 'bioID_melvich01w', 'bioID_mendigi01w', 'bioID_miaomi01w', 'bioID_millebr01w', 'bioID_milleco01w', 'bioID_milleke01w', 'bioID_millsta01w', 'bioID_miltode01w', 'bioID_mitchle01w', 'bioID_moeggli01w', 'bioID_moisead01w', 'bioID_montaan01w', 'bioID_montgre01w', 'bioID_mooreja01w', 'bioID_mooreje01w', 'bioID_moorelo01w', 'bioID_moorena01w', 'bioID_mooreta01w', 'bioID_moosca01w', 'bioID_mosbybe01w', 'bioID_moweje01w', 'bioID_mulitna01w', 'bioID_murphsh01w', 'bioID_nagyan01w', 'bioID_nanch01w', 'bioID_ndiayas01w', 'bioID_ndongem01w', 'bioID_nemcoev01w', 'bioID_nevescl01w', 'bioID_newtoch01w', 'bioID_ngarsch01w', 'bioID_ngoyibe01w', 'bioID_nieuwma01w', 'bioID_nnamach01w', 'bioID_nolande01w', 'bioID_nygaava01w', 'bioID_ogayu01w', 'bioID_ohldeni01w', 'bioID_oneilkr01w', 'bioID_osipoir01w', 'bioID_owenhe01w', 'bioID_owenssh01w', 'bioID_pageda01w', 'bioID_pagemu01w', 'bioID_paigeyo01w', 'bioID_paliesa01w', 'bioID_palmewe01w', 'bioID_parisco01w', 'bioID_parkeca01w', 'bioID_pascafl01w', 'bioID_pavlimi01w', 'bioID_payeka01w', 'bioID_penicti01w', 'bioID_pennjo01w', 'bioID_perkiji01w', 'bioID_pettibr01w', 'bioID_philler01w', 'bioID_phillta01w', 'bioID_pierspl01w', 'bioID_pondeca01w', 'bioID_potthan01w', 'bioID_powelel01w', 'bioID_powelni01w', 'bioID_pricear01w', 'bioID_pridely01w', 'bioID_pringla01w', 'bioID_queenbr01w', 'bioID_quiglal01w', 'bioID_quinnno01w', 'bioID_quinnte01w', 'bioID_radunha01w', 'bioID_raglafe01w', 'bioID_randase01w', 'bioID_rasmukr01w', 'bioID_raymost01w', 'bioID_reddja01w', 'bioID_reedbr01w', 'bioID_reidtr01w', 'bioID_rileyru01w', 'bioID_rizzoje01w', 'bioID_robinas01w', 'bioID_robincr01w', 'bioID_robinre01w', 'bioID_rolanja01w', 'bioID_rossad01w', 'bioID_rushle01w', 'bioID_salesny01w', 'bioID_sampsch01w', 'bioID_samsh01w', 'bioID_sanchis01w', 'bioID_sandeam01w', 'bioID_sanfona01w', 'bioID_sanniol01w', 'bioID_santoal01w', 'bioID_santoke01w', 'bioID_sarenra01w', 'bioID_sauerpa01w', 'bioID_saundja01w', 'bioID_saureau01w', 'bioID_schumke01w', 'bioID_schwege01w', 'bioID_scottol01w', 'bioID_shakiel01w', 'bioID_sharpkb01w', 'bioID_shielas01w', 'bioID_slaisma01w', 'bioID_slauggw01w', 'bioID_slavtge01w', 'bioID_smithai01w', 'bioID_smithbr01w', 'bioID_smithch02w', 'bioID_smithch03w', 'bioID_smithcr01w', 'bioID_smithje01w', 'bioID_smithka01w', 'bioID_smithki01w', 'bioID_smithla01w', 'bioID_smithta01w', 'bioID_smithty01w', 'bioID_smithwa01w', 'bioID_snellbe01w', 'bioID_snowmi01w', 'bioID_spencsi01w', 'bioID_spornra01w', 'bioID_stafftr01w', 'bioID_staleda01w', 'bioID_stansti01w', 'bioID_starbka01w', 'bioID_stedika01w', 'bioID_stepama01w', 'bioID_stephst01w', 'bioID_stevema01w', 'bioID_stileja01w', 'bioID_stinsan01w', 'bioID_stiresh01w', 'bioID_stockta01w', 'bioID_streiju01w', 'bioID_strotan01w', 'bioID_summela01w', 'bioID_sun-mju01w', 'bioID_suttota01w', 'bioID_swanike01w', 'bioID_swoopsh01w', 'bioID_tateso01w', 'bioID_tauradi01w', 'bioID_tayloli01w', 'bioID_taylope01w', 'bioID_teaslni01w', 'bioID_teilaza01w', 'bioID_terryka01w', 'bioID_thomaca01w', 'bioID_thomach01w', 'bioID_thomala01w', 'bioID_thomast01w', 'bioID_thompal01w', 'bioID_thompti01w', 'bioID_thorbsh01w', 'bioID_thorner01w', 'bioID_threaro01w', 'bioID_tilliic01w', 'bioID_timmsmi01w', 'bioID_tolivkr01w', 'bioID_torniel01w', 'bioID_torrele01w', 'bioID_traviti01w', 'bioID_tremich01w', 'bioID_turneba01w', 'bioID_tuvicsl01w', 'bioID_udokamf01w', 'bioID_ujhelpe01w', 'bioID_umoh-it01w', 'bioID_valdeam01w', 'bioID_vangomi01w', 'bioID_vaughki01w', 'bioID_vaughkr01w', 'bioID_vealkr01w', 'bioID_vilipda01w', 'bioID_vodicka01w', 'bioID_vodopna01w', 'bioID_walkeas01w', 'bioID_walkeay01w', 'bioID_walkede01w', 'bioID_walkema01w', 'bioID_walsema01w', 'bioID_washico01w', 'bioID_washito01w', 'bioID_wautean01w', 'bioID_weathte01w', 'bioID_webbum01w', 'bioID_weberma01w', 'bioID_weckeke01w', 'bioID_whaleli01w', 'bioID_whitede01w', 'bioID_whiteer01w', 'bioID_whiteta01w', 'bioID_whitiva01w', 'bioID_whitmta01w', 'bioID_whittje01w', 'bioID_whittkh01w', 'bioID_wickssu01w', 'bioID_widemja01w', 'bioID_wiggica01w', 'bioID_wilkibr01w', 'bioID_williad01w', 'bioID_willian01w', 'bioID_willibe01w', 'bioID_willile01w', 'bioID_willile02w', 'bioID_willili01w', 'bioID_willina01w', 'bioID_williri01w', 'bioID_willish01w', 'bioID_willita01w', 'bioID_willito01w', 'bioID_williwe01w', 'bioID_wilsoam01w', 'bioID_wirthch01w', 'bioID_wisdoli01w', 'bioID_witheso01w', 'bioID_wolteka01w', 'bioID_wolvean01w', 'bioID_wrighsh01w', 'bioID_wrighta01w', 'bioID_wyckobr01w', 'bioID_wynneda01w', 'bioID_yamasli01w', 'bioID_yilmane01w', 'bioID_youngca01w', 'bioID_youngso01w', 'bioID_youngta01w', 'bioID_zakalok01w', 'bioID_zarafr01w', 'bioID_zellosh01w', 'bioID_zirkozu01w', 'bioID_zollsh01w', 'pos_C', 'pos_C-F', 'pos_F', 'pos_F-C', 'pos_F-G', 'pos_G', 'pos_G-F', 'college_', 'college_Academy of Sport Moscow', 'college_Alabama', 'college_Alabama-Birmingham', 'college_Arizona', 'college_Arizona State', 'college_Arkansas', 'college_Arkansas State', 'college_Auburn', 'college_Australian Institute of Sport', 'college_Baylor', 'college_Boise State', 'college_Boston College', 'college_Brigham Young', 'college_California', 'college_Central Florida', 'college_Cincinnati', 'college_Clemson', 'college_Colorado', 'college_Colorado State', 'college_Connecticut', 'college_DePaul', 'college_Delaware', 'college_Denver', 'college_Duke', 'college_Duquesne', 'college_Flordia', 'college_Florida', 'college_Florida Atlantic', 'college_Florida International', 'college_Florida State', 'college_Furman', 'college_George Mason', 'college_George Washington', 'college_Georgetown', 'college_Georgia', 'college_Georgia Tech', 'college_Harvard', \"college_Hawai'i\", 'college_Houston', 'college_Howard', 'college_Illinois', 'college_Illinois State', 'college_Indiana', 'college_Iona', 'college_Iowa', 'college_Iowa State', 'college_James Madison', 'college_Kansas', 'college_Kansas State', 'college_Kentucky', 'college_Liberty', 'college_Long Beach State', 'college_Louisiana State', 'college_Louisiana Tech', 'college_Louisville', 'college_Maine', 'college_Maryland', 'college_Memphis', 'college_Miami (FL)', 'college_Michigan', 'college_Michigan State', 'college_Middle Tennessee State', 'college_Minnesota', 'college_Mississippi', 'college_Mississippi State', 'college_Missouri', 'college_Missouri State', 'college_Montana State', 'college_Nebraska', 'college_New Mexico', 'college_North Carolina', 'college_North Carolina State', 'college_North Carolina State (jr-sr)', 'college_Northern Illinois', 'college_Notre Dame', 'college_Ohio State', 'college_Oklahoma', 'college_Old Dominion', 'college_Oregon', 'college_Oregon State', 'college_Penn State', 'college_Pepperdine', 'college_Pittsburgh', 'college_Providence', 'college_Purdue', 'college_Rice', 'college_Rutgers', 'college_SE Oklahoma State', 'college_SW Tennessee CC', 'college_Seton Hall', 'college_South Carolina', 'college_Southern Mississippi', 'college_Southern Nazarene', 'college_St. Edwards', \"college_St. Joseph's\", 'college_Stanford', 'college_Syracuse', 'college_Temple', 'college_Tennessee', 'college_Texas', 'college_Texas A&M', 'college_Texas Christian', 'college_Texas Tech', 'college_Tulane', 'college_U. Western Sydney', 'college_UC Santa Barbara', 'college_UCLA', 'college_UNC Charlotte', 'college_UNLV', 'college_USC', 'college_University of Sydney', 'college_Utah', 'college_Valparaiso', 'college_Vanderbilt', 'college_Virginia', 'college_Virginia Commonwealth', 'college_Virginia Tech', 'college_Washington', 'college_West Virginia', 'college_Western Illinois', 'college_Western Kentucky', 'college_Wisconsin', 'college_Wisconsin-Green Bay', 'college_Xavier', 'college_none', 'collegeOther_', 'collegeOther_Arkansas-Fort Smith (fr-so)', 'collegeOther_Florida', 'collegeOther_Grayson County (JC)', 'collegeOther_NE Oklahoma A&M (JC)', 'collegeOther_North Carolina State', 'tmID_ATL', 'tmID_CHA', 'tmID_CHI', 'tmID_CLE', 'tmID_CON', 'tmID_DET', 'tmID_HOU', 'tmID_IND', 'tmID_LAS', 'tmID_MIA', 'tmID_MIN', 'tmID_NYL', 'tmID_ORL', 'tmID_PHO', 'tmID_POR', 'tmID_SAC', 'tmID_SAS', 'tmID_SEA', 'tmID_UTA', 'tmID_WAS']\n"
     ]
    }
   ],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df['birthDate'])\n",
    "\n",
    "# # get all rows from df where year = 10\n",
    "test_data = df.loc[df[\"year\"] == 10]\n",
    "\n",
    "# # get all rows from df where year <> 10\n",
    "train_data = df.loc[df[\"year\"] != 10]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001))\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# test model\n",
    "predictions = model.predict(test_inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all predictions to integers\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        predictions[i][j] = int(round(predictions[i][j]))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    # print(\"player: \", i)\n",
    "    print(\"player: \", player_ids_10[i])\n",
    "    for j in range(len(predictions[i])):\n",
    "        print(labels[j], \":  predicted: \", predictions[i][j], \" actual: \", test_labels[i][j])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try neuronal model \n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from teams;\", connection)\n",
    "print(df.columns)\n",
    "\n",
    "# transform categorical data\n",
    "categorical_columns = [\"tmID\", \"firstRound\", \"semis\", \"finals\",\"name\",\"arena\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# get inputs and outputs\n",
    "inputs = df.loc[:, df.columns != \"playoff\"].values\n",
    "labels = df[\"playoff\"].values\n",
    "\n",
    "# split data\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.3, random_state=1)\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "# create model\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# predict\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
