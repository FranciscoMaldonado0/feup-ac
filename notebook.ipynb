{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import sqlite3\n",
    "import subprocess\n",
    "\n",
    "# Create a shell script\n",
    "# with open('myscript.sh', 'w') as f:\n",
    "#     f.write('cat ./database/final.sql | sqlite3 ./database/bdfinal.sql')\n",
    "\n",
    "# Execute the script in WSL\n",
    "# subprocess.run([\"wsl\", \"./myscript.sh\"], check=True,shell=True)\n",
    "\n",
    "connection = sqlite3.connect(\"./database/bdfinal.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM coaches;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "## ---//---\n",
    "\n",
    "## get rows where pos = \"\"\n",
    "df = pd.read_sql(\"SELECT * FROM players;\",connection)\n",
    "\n",
    "## iterate through rows\n",
    "\n",
    "col_names = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ## iterate through columns\n",
    "    found = False\n",
    "    if row[\"pos\"] == \"\":\n",
    "        found = True\n",
    "    elif row[\"height\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"weight\"] == 0:\n",
    "        found = True\n",
    "    elif row[\"birthDate\"] == \"\" or row[\"birthDate\"] == \"0000-00-00\":\n",
    "        found = True\n",
    "    elif row[\"college\"] == \"\":\n",
    "        found = True\n",
    "\n",
    "    if(found):\n",
    "        print(row[\"bioID\"])\n",
    "    \n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM players_teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM series_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)\n",
    "\n",
    "print(\"\\n=========================================\\n\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM teams_post;\",connection)\n",
    "\n",
    "null_mask = df.isnull().any(axis=1)\n",
    "null_rows = df[null_mask]\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player\n",
    "\n",
    "- After a quick glance at the data, it's easy to see that there's a certain amount of players that have many important missing/null values (college, height and weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select bioID from players where weight = 0 or height = 0 or college = '' or pos = '';\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets check if any of these players with a null position are actually coaches, since the awards_players tables has a coach award and references the players table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select count(*) from players where pos = \"\";\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '';\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n",
    "\n",
    "# select count(*) from players where pos = \"\" and bioID in (select coachID from coaches);\n",
    "# execute the query\n",
    "df = pd.read_sql(\"select count(*) from players where pos = '' and bioID in (select coachID from coaches);\",connection)\n",
    "num_players = df.values[0][0]\n",
    "print(num_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 52 out of the 78 players without position are coaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these 208 players, it's important to see which actually were a part of a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_missing_values_players = pd.read_sql(\"select distinct(bioID), weight, height, pos from players where (weight = 0 or height = 0) and pos <> ''\", connection)\n",
    "print(active_missing_values_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regarding these 83 players, if a player doesn't have their position missing, we decided to replace their missing weight and/or height values with the average value of the players of their same position. \n",
    "\n",
    "    - Obtain the average weight and height for each player position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query para cada valor\n",
    "avg_pos_weights = pd.read_sql(\"select pos, avg(weight) from players where weight <> 0 group by pos;\", connection)\n",
    "print(avg_pos_weights)\n",
    "avg_pos_heights = pd.read_sql(\"select pos, avg(height) from players where height <> 0 group by pos;\", connection)\n",
    "print(avg_pos_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Store the values in two dictionaries, where the key values are the players' positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to a dictionary where the key are the positions and the values are the avg weights\n",
    "avg_weights = {}\n",
    "\n",
    "for index, row in avg_pos_weights.iterrows():\n",
    "    avg_weights[row[\"pos\"]] = row[\"avg(weight)\"]\n",
    "    \n",
    "print(avg_weights)\n",
    "\n",
    "avg_heights = {}\n",
    "\n",
    "for index, row in avg_pos_heights.iterrows():\n",
    "    avg_heights[row[\"pos\"]] = row[\"avg(height)\"]\n",
    "    \n",
    "print(avg_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in active_missing_values_players.iterrows():\n",
    "    player = pd.read_sql(\"select * from players where bioID = '\" + row[\"bioID\"] + \"';\", connection)\n",
    "    \n",
    "    pos = player[\"pos\"].values[0]\n",
    "    if pos == '':\n",
    "        continue\n",
    "    \n",
    "    if(player[\"weight\"] != 0 and player[\"height\"] != 0):\n",
    "        # print(\"Player already has values\")\n",
    "        # print(player)\n",
    "        continue\n",
    "    \n",
    "    print(player)\n",
    "    \n",
    "    print(\"\\n===\\n\")\n",
    "\n",
    "    ## get average values for the player's position pos\n",
    "    if(player[\"weight\"].values[0] == 0):\n",
    "        weight = avg_weights[pos]\n",
    "    else:\n",
    "        weight = player[\"weight\"].values[0]\n",
    "    if (player[\"height\"].values[0] == 0):\n",
    "        height = avg_heights[pos]\n",
    "    else:\n",
    "        height = player[\"height\"].values[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    ## get row index\n",
    "    pos = row.index[0]\n",
    "    \n",
    "    # update player's height and weight\n",
    "    print(\"UPDATE players SET height = '\" + str(height) + \"', weight = '\" + str(weight) + \"' WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    \n",
    "    # update player's height and weight\n",
    "    connection.execute(\"UPDATE players SET height = \" + str(height) + \", weight = \" + str(weight) + \" WHERE bioID = '\" + player[\"bioID\"].values[0] + \"';\")\n",
    "    connection.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check if there are any outliers in the data\n",
    "\n",
    "## Player\n",
    "\n",
    "In this table we will be looking for outliers in weight, height and birth dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player weight distribution\n",
    "df = pd.read_sql(\"SELECT weight FROM players;\",connection)\n",
    "df = df[df.weight != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player weight distribution\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"weight\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player height distribution\n",
    "df = pd.read_sql(\"SELECT height FROM players;\",connection)\n",
    "df = df[df.height != 0]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player height distribution\")\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"height\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there is one player with a height of 9.0. We can fix this via mean imputation, which means that her height will be replaced by the average height of the players that play in the same position as her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select player with height < 20\n",
    "df = pd.read_sql(\"SELECT * FROM players WHERE height < 20 and height > 0;\",connection)\n",
    "\n",
    "# get the average height for the player's position\n",
    "average_height = avg_heights[df[\"pos\"].values[0]]\n",
    "\n",
    "# update player's height\n",
    "connection.execute(\"UPDATE players SET height = \" + str(average_height) + \" WHERE bioID = '\" + df[\"bioID\"].values[0] + \"';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with player birth year distribution\n",
    "df = pd.read_sql(\"SELECT birthDate FROM players;\",connection)\n",
    "df = df[df.birthDate != \"0000-00-00\"]\n",
    "\n",
    "#convert birthdate to year\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Player birth year distribution\")\n",
    "plt.xlabel(\"Birth year\")\n",
    "plt.ylabel(\"Number of players\")\n",
    "plt.boxplot(df[\"birthDate\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent data\n",
    "\n",
    "### Player Awards\n",
    "\n",
    "- Check if there's any award, that should be given to one player, is given to two or more players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\" select count(playerID), award, year from awards_players group by award, year;\", connection)\n",
    "\n",
    "# print rows \n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we noticed that there's an award missing part of its title. Therefore, we'll have to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute(\"UPDATE awards_players SET award = 'Kim Perrot Sportsmanship Award' WHERE award = 'Kim Perrot Sportsmanship';\")\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams Post\n",
    "\n",
    "- Check if, in any year, no more than 8 teams passed to the playoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select count(tmID) as num, year from teams_post group by year having num > 8;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if, in any year, only one team won the playoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, finals from teams where finals = 'W' order by year;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teams\n",
    "\n",
    "- Check if the sum of games won and lost by a player is equal to the total games played by a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID, won, lost, GP, (won + lost) as Games from teams where Games <> GP;\", connection)\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the sum of rebounds made by a team is equal to the sum of offensive rebounds and defensive rebounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_sql(\"select year, tmID, o_oreb, o_dreb, o_reb, (o_oreb + o_dreb) as rebounds from teams where o_reb <> rebounds;\", connection)\n",
    "print(dataFrame)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataFrame = pd.read_sql(\"select year, tmID, d_oreb, d_dreb, d_reb, (d_oreb + d_dreb) as rebounds from teams where d_reb <> rebounds;\", connection)\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if the stats (field goals, 3 pointers, free throws, etc.) attempted are in a bigger quantity than the stats made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_fgm > o_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_ftm > o_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where o_3pm > o_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_fgm > d_fga;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_ftm > d_fta;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n",
    "\n",
    "dataframe = pd.read_sql(\"select year, tmID from teams where d_3pm > d_3pa;\", connection)\n",
    "print(dataframe)\n",
    "print(\"===============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN tmTRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmORB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmDRB;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN opptmTRB;\")\n",
    "\n",
    "# remove franchID and lgID from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN franchID;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove firstSeason and lastSeason from players using a query\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN firstSeason;\")\n",
    "connection.execute(\"ALTER TABLE players DROP COLUMN lastSeason;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgIDWinner, lgIDLoser and series from series_post using a query\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDWinner;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN lgIDLoser;\")\n",
    "connection.execute(\"ALTER TABLE series_post DROP COLUMN series;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from teams_post using a query\n",
    "connection.execute(\"ALTER TABLE teams_post DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from awards_players using a query\n",
    "connection.execute(\"ALTER TABLE awards_players DROP COLUMN lgID;\")\n",
    "\n",
    "#remove lgID from players_teams using a query\n",
    "connection.execute(\"ALTER TABLE players_teams DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lgID from coaches using a query\n",
    "connection.execute(\"ALTER TABLE coaches DROP COLUMN lgID;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dataframe without any column names, indices or data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "## obtain the team IDS and year\n",
    "team_ids = pd.read_sql(\"select tmID, year, confID from teams order by tmID\", connection)\n",
    "\n",
    "\n",
    "\n",
    "## iterate through each team and year\n",
    "for index, row in team_ids.iterrows():\n",
    "\n",
    "    ## obtain the team ID and year for each row\n",
    "    team_id = row['tmID']\n",
    "    year = row['year']\n",
    "    confID = row['confID']\n",
    "\n",
    "    # get all players from the team and year\n",
    "    query = \"select tmID, year, playerID from players_teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "\n",
    "    ## obtain the players for each team\n",
    "    team_players = pd.read_sql(query, connection)\n",
    "    \n",
    "    team_stats = {\"year\": year, \"points\": 0, \"oRebounds\": 0, \"dRebounds\": 0, \"rebounds\": 0,\n",
    "                  \"assists\": 0, \"steals\": 0, \"blocks\": 0, \"turnovers\": 0, \"PF\": 0, \"fgAttempted\": 0,\n",
    "                  \"fgMade\": 0, \"ftAttempted\": 0, \"ftMade\": 0, \"threeAttempted\": 0, \"threeMade\": 0,\n",
    "                  'weight': 0, 'height': 0, \"player_awards\": 0, \"confID\": confID, \"num_playoffs\": 0,\n",
    "                  \"num_semis\": 0, \"num_finals\": 0, \"coach_win_ratio\": 0, \"coach_stint\": 0,\n",
    "                  \"playoff\": 0}\n",
    "\n",
    "    # iterate through each player\n",
    "    for idx, r in team_players.iterrows():\n",
    "        ## obtain the player ID for each row\n",
    "        player_id = r['playerID']\n",
    "\n",
    "        # get the player's position\n",
    "        query = \"select pos from players where bioID = '\" + player_id + \"';\"\n",
    "        pos = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        # get number of years played\n",
    "        query = \"select count(distinct year) as num_years from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        num_years = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        if num_years == 0:\n",
    "            query = \"select avg(points) as points, avg(oRebounds) as oRebounds, avg(dRebounds) as dRebounds, avg(rebounds) as rebounds, avg(assists) as assists, \\\n",
    "                avg(steals) as steals, avg(blocks) as blocks, avg(turnovers) as turnovers, avg(PF) as PF, avg(fgAttempted) as fgAttempted, \\\n",
    "                avg(fgMade) as fgMade, avg(ftAttempted) as ftAttempted, avg(ftMade) as ftMade, avg(threeAttempted) as threeAttempted, avg(threeMade) as threeMade \\\n",
    "                from players_teams join players on players_teams.playerID = players.bioID where year = \" + str(year - 1) + \" \\\n",
    "                and pos = '\" + pos + \"' and playerID not in (select playerID from players_teams where year < \" + str(year - 1) + \");\"\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            ## add the player stats to the team stats\n",
    "            team_stats['points'] += (player_stats['points'].values[0] or 0 )\n",
    "            team_stats['oRebounds'] += (player_stats['oRebounds'].values[0] or 0 )\n",
    "            team_stats['dRebounds'] += (player_stats['dRebounds'].values[0] or 0 )\n",
    "            team_stats['rebounds'] += (player_stats['rebounds'].values[0] or 0 )\n",
    "            team_stats['assists'] += (player_stats['assists'].values[0] or 0 )\n",
    "            team_stats['steals'] += (player_stats['steals'].values[0] or 0 )\n",
    "            team_stats['blocks'] += (player_stats['blocks'].values[0] or 0 )\n",
    "            team_stats['turnovers'] += (player_stats['turnovers'].values[0] or 0 )\n",
    "            team_stats['PF'] += (player_stats['PF'].values[0] or 0 )\n",
    "            team_stats['fgAttempted'] += (player_stats['fgAttempted'].values[0] or 0 )\n",
    "            team_stats['fgMade'] += (player_stats['fgMade'].values[0] or 0 )\n",
    "            team_stats['ftAttempted'] += (player_stats['ftAttempted'].values[0] or 0 )\n",
    "            team_stats['ftMade'] += (player_stats['ftMade'].values[0] or 0 )\n",
    "            team_stats['threeAttempted'] += (player_stats['threeAttempted'].values[0] or 0 )\n",
    "            team_stats['threeMade'] += (player_stats['threeMade'].values[0] or 0 )\n",
    "            \n",
    "        else:\n",
    "\n",
    "            query = \"select year, points, oRebounds, dRebounds, rebounds, assists, \\\n",
    "                    steals, blocks, turnovers, PF, fgAttempted, \\\n",
    "                    fgMade, ftAttempted, ftMade, threeAttempted, sum(threeMade) as threeMade \\\n",
    "                    from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        \n",
    "            ## obtain the average stats for each team\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            points = oRebounds = dRebounds = rebounds = assists = steals = blocks = turnovers = PF = fgAttempted = fgMade = ftAttempted = ftMade = threeAttempted = threeMade = 0\n",
    "            sum_weight = 0\n",
    "            # iterate player stats and make weighted average for each year\n",
    "            for j, row2 in player_stats.iterrows():\n",
    "\n",
    "                iteration_year = row2['year']\n",
    "\n",
    "                weight = 1 / (year - iteration_year)\n",
    "\n",
    "                sum_weight += weight\n",
    "\n",
    "                points += row2['points'] * weight\n",
    "                oRebounds += row2['oRebounds'] * weight\n",
    "                dRebounds += row2['dRebounds'] * weight\n",
    "                rebounds += row2['rebounds'] * weight\n",
    "                assists += row2['assists'] * weight\n",
    "                steals += row2['steals'] * weight\n",
    "                blocks += row2['blocks'] * weight\n",
    "                turnovers += row2['turnovers'] * weight\n",
    "                PF += row2['PF'] * weight\n",
    "                fgAttempted += row2['fgAttempted'] * weight\n",
    "                fgMade += row2['fgMade'] * weight\n",
    "                ftAttempted += row2['ftAttempted'] * weight\n",
    "                ftMade += row2['ftMade'] * weight\n",
    "                threeAttempted += row2['threeAttempted'] * weight\n",
    "                threeMade += row2['threeMade'] * weight\n",
    "\n",
    "            # add the player stats to the team stats\n",
    "            team_stats['points'] += points / sum_weight\n",
    "            team_stats['oRebounds'] += oRebounds / sum_weight\n",
    "            team_stats['dRebounds'] += dRebounds / sum_weight\n",
    "            team_stats['rebounds'] += rebounds / sum_weight\n",
    "            team_stats['assists'] += assists / sum_weight\n",
    "            team_stats['steals'] += steals / sum_weight\n",
    "            team_stats['blocks'] += blocks / sum_weight\n",
    "            team_stats['turnovers'] += turnovers / sum_weight\n",
    "            team_stats['PF'] += PF / sum_weight\n",
    "            team_stats['fgAttempted'] += fgAttempted / sum_weight\n",
    "            team_stats['fgMade'] += fgMade / sum_weight\n",
    "            team_stats['ftAttempted'] += ftAttempted / sum_weight\n",
    "            team_stats['ftMade'] += ftMade / sum_weight\n",
    "            team_stats['threeAttempted'] += threeAttempted / sum_weight\n",
    "            team_stats['threeMade'] += threeMade / sum_weight\n",
    "        \n",
    "        num_years = max(num_years, 1)\n",
    "\n",
    "\n",
    "        # get each player num awards\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + player_id + \"' and ap.year < \" + str(year) + \";\"\n",
    "        num_awards_player = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['player_awards'] += num_awards_player\n",
    "\n",
    "        # get the number of times the player went to the playoffs\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and firstRound <> '';\"\n",
    "        num_playoffs = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_playoffs'] += num_playoffs\n",
    "\n",
    "        # get the number of times the player went to the semis\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and semis <> '';\"\n",
    "        num_semis = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_semis'] += num_semis\n",
    "\n",
    "        # get the number of times the player went to the finals\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and finals <> '';\"\n",
    "        num_finals = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_finals'] += num_finals\n",
    "    \n",
    "    \n",
    "    query = \"select avg(weight), avg(height) from players_teams join players on players.bioID = players_teams.playerID \\\n",
    "        where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "        \n",
    "    ## obtain the average weight and height for each team\n",
    "    team_weight_height = pd.read_sql(query, connection)\n",
    "    \n",
    "    ## add the average weight and height to the team stats\n",
    "    team_stats['weight'] = team_weight_height['avg(weight)'].values[0]\n",
    "    team_stats['height'] = team_weight_height['avg(height)'].values[0]\n",
    "\n",
    "    # get team coach\n",
    "    query = \"select coachID, stint from coaches where tmID = '\" + team_id + \"' and year = \"+ str(year) +\";\"\n",
    "    coach_id = pd.read_sql(query, connection).values[0][0]\n",
    "    stint = pd.read_sql(query, connection).values[0][1]\n",
    "    team_stats['coach_stint'] = stint\n",
    "    \n",
    "    \n",
    "    query = \"select count(award) as num_awards from coaches join awards_players \\\n",
    "              on coaches.coachID = awards_players.playerID and coaches.year = awards_players.year \\\n",
    "              where coachID = '\" + coach_id + \"' and coaches.year < \" + str(year) + \";\"\n",
    "    coach_awards = pd.read_sql(query, connection).values[0][0]\n",
    "    team_stats['coach_awards'] = coach_awards\n",
    "\n",
    "    # get average wins and losses from coach\n",
    "    query = \"select avg(won), avg(lost) from coaches where coachID = '\" + coach_id + \"' and year < \" + str(year) + \";\"\n",
    "    coach_wins_losses = pd.read_sql(query, connection)\n",
    "    num_matches_coach = (coach_wins_losses['avg(won)'].values[0] or 0) + (coach_wins_losses['avg(lost)'].values[0] or 0)\n",
    "    if num_matches_coach == 0:\n",
    "        team_stats['coach_win_ratio'] = 0\n",
    "    else:\n",
    "        team_stats['coach_win_ratio'] = (coach_wins_losses['avg(won)'].values[0] or 0) / num_matches_coach\n",
    "        \n",
    "        \n",
    "    query = \"select playoff from teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "    playoff = pd.read_sql(query, connection)\n",
    "    \n",
    "    # check if it's Y or N\n",
    "    if playoff['playoff'][0] == 'Y':\n",
    "        team_stats['playoff'] = 1\n",
    "    else:\n",
    "        team_stats['playoff'] = 0\n",
    "        \n",
    "    \n",
    "    # append the stats to the dataframe\n",
    "    df = df._append(team_stats, ignore_index=True)\n",
    "\n",
    "    print(\"Team id: \" + team_id)\n",
    "    print(team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"confID\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df= pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_year = 10\n",
    "\n",
    "# # get all rows from df where year = target year\n",
    "test_data = df.loc[df[\"year\"] == target_year]\n",
    "\n",
    "# # get all rows from df where year <> target_year and year <> 1\n",
    "train_data = df.loc[df[\"year\"] < target_year]\n",
    "train_data = train_data.loc[train_data[\"year\"] != 1]\n",
    "\n",
    "labels = ['playoff']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "print(train_inputs)\n",
    "print(train_labels)\n",
    "\n",
    "print(test_inputs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "logreg.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "feature_importance = abs(logreg.coef_[0])\n",
    "feature_importance = sorted(zip(inputs, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature importance:\")\n",
    "for i in range(len(feature_importance)):\n",
    "    print(f\"{feature_importance[i][0]}: {feature_importance[i][1]}\")\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(test_labels, logreg.predict(test_inputs))\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, logreg.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "query = \"select tmId, confID from teams where year = \" + str(target_year) + \";\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "team_ids = pd.read_sql(query, connection)\n",
    "team_ids = team_ids[\"tmID\"].values\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=2000, alpha=0.0001, solver='adam', verbose=0, random_state=21, tol=0.000000001)\n",
    "mlp.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = mlp.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = mlp.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "knn.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = knn.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = knn.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "# connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stINTEGER', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_10 = df[df[\"year\"] == 10][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bioID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bioID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilizador\\Documents\\M.EIC\\AC\\feup-ac\\notebook.ipynb Cell 52\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y330sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m categorical_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbioID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcollege\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcollegeOther\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtmID\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y330sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m categorical_columns:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y330sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y330sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mget_dummies(df, columns\u001b[39m=\u001b[39mcategorical_columns)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y330sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mbirthDate\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bioID'"
     ]
    }
   ],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df['birthDate'])\n",
    "\n",
    "# # get all rows from df where year = 10\n",
    "test_data = df.loc[df[\"year\"] == 10]\n",
    "\n",
    "# # get all rows from df where year <> 10\n",
    "train_data = df.loc[df[\"year\"] != 10]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001))\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# test model\n",
    "predictions = model.predict(test_inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all predictions to integers\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        predictions[i][j] = int(round(predictions[i][j]))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    # print(\"player: \", i)\n",
    "    print(\"player: \", player_ids_10[i])\n",
    "    for j in range(len(predictions[i])):\n",
    "        print(labels[j], \":  predicted: \", predictions[i][j], \" actual: \", test_labels[i][j])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try neuronal model \n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from teams;\", connection)\n",
    "print(df.columns)\n",
    "\n",
    "# transform categorical data\n",
    "categorical_columns = [\"tmID\", \"firstRound\", \"semis\", \"finals\",\"name\",\"arena\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# get inputs and outputs\n",
    "inputs = df.loc[:, df.columns != \"playoff\"].values\n",
    "labels = df[\"playoff\"].values\n",
    "\n",
    "# split data\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.3, random_state=1)\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "# create model\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001)\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# predict\n",
    "predictions = model.predict(test_inputs)\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season 11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this part is adding the season 11 data to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/coaches.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO coaches (coachID, year, tmID, stint) VALUES ('\" + row[0] + \"', '\" + str(row[1]) + \"', '\" + row[2] +  \"', '\" + str(row[4]) + \"');\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/teams.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO teams (year, tmID, confID, name, arena) VALUES ('\" + str(row[0]) + \"', '\" + row[2] + \"', '\" + row[4] +  \"', '\" + row[5] + \"', '\" + row[6] + \"');\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the season 11 data to the database\n",
    "\n",
    "# get the data from the csv file\n",
    "df = pd.read_csv(\"./season11/players_teams.csv\")\n",
    "\n",
    "# iterate through each row and add the data to the database\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "    # add the row to the database\n",
    "    connection.execute(\"INSERT INTO players_teams (playerID, year, stINTEGER, tmID) VALUES ('\" + row[0] + \"', '\" + str(row[1]) + \"', '\" + str(row[2]) +  \"', '\" + row[3] + \"');\")\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# remove confID and divID from teams using a query\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN confID;\")\n",
    "# connection.execute(\"ALTER TABLE teams DROP COLUMN divID;\")\n",
    "# connection.commit()\n",
    "\n",
    "# get data\n",
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stINTEGER', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_11 = df[df[\"year\"] == 11][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      height  weight  birthDate  year  stINTEGER  points  oRebounds   \n",
      "0       74.0   169.0       1980     2          0     343         43  \\\n",
      "1       74.0   169.0       1980     3          0     314         45   \n",
      "2       74.0   169.0       1980     4          0     318         44   \n",
      "3       74.0   169.0       1980     5          0     146         17   \n",
      "4       74.0   169.0       1980     6          0     304         29   \n",
      "...      ...     ...        ...   ...        ...     ...        ...   \n",
      "1871    78.0   174.0       1977     3          2       6          0   \n",
      "1872    70.0   146.0       1976     6          0      90         11   \n",
      "1873    70.0   155.0       1986    10          0     406         25   \n",
      "1874    69.0   145.0       1980     4          0      11          0   \n",
      "1875    67.0   148.0       1986     9          0      10          1   \n",
      "\n",
      "      dRebounds  rebounds  assists  ...  tmID_MIN  tmID_NYL  tmID_ORL   \n",
      "0           131       174       53  ...      True     False     False  \\\n",
      "1           101       146       60  ...      True     False     False   \n",
      "2            97       141       82  ...      True     False     False   \n",
      "3            57        74       45  ...      True     False     False   \n",
      "4            78       107       60  ...      True     False     False   \n",
      "...         ...       ...      ...  ...       ...       ...       ...   \n",
      "1871          3         3        0  ...     False     False     False   \n",
      "1872         28        39       51  ...     False     False     False   \n",
      "1873         81       106       62  ...     False     False     False   \n",
      "1874          2         2        1  ...     False     False     False   \n",
      "1875          2         3        3  ...      True     False     False   \n",
      "\n",
      "      tmID_PHO  tmID_POR  tmID_SAC  tmID_SAS  tmID_SEA  tmID_UTA  tmID_WAS  \n",
      "0        False     False     False     False     False     False     False  \n",
      "1        False     False     False     False     False     False     False  \n",
      "2        False     False     False     False     False     False     False  \n",
      "3        False     False     False     False     False     False     False  \n",
      "4        False     False     False     False     False     False     False  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1871      True     False     False     False     False     False     False  \n",
      "1872     False     False     False     False      True     False     False  \n",
      "1873     False     False     False     False     False     False     False  \n",
      "1874     False     False     False     False     False     False      True  \n",
      "1875     False     False     False     False     False     False     False  \n",
      "\n",
      "[1876 rows x 735 columns]\n",
      "Empty DataFrame\n",
      "Columns: [height, weight, birthDate, year, stINTEGER, points, oRebounds, dRebounds, rebounds, assists, steals, blocks, turnovers, PF, fgAttempted, fgMade, ftAttempted, ftMade, threeAttempted, threeMade, num_awards_player, bioID_abrossv01w, bioID_adamsjo01w, bioID_aguilel01w, bioID_ajavoma01w, bioID_aldrima01w, bioID_alhalta01w, bioID_allench01w, bioID_amachma01w, bioID_ambermo01w, bioID_anderam01w, bioID_anderch01w, bioID_anderjo01w, bioID_anderke01w, bioID_andrame01w, bioID_anosini01w, bioID_arcaija01w, bioID_askamma01w, bioID_atkinla01w, bioID_atunrmo01w, bioID_augusse01w, bioID_aycocan01w, bioID_azizle01w, bioID_azzije01w, bioID_badertr01w, bioID_bakersh01w, bioID_balesal01w, bioID_banchrh01w, bioID_baranel01w, bioID_barksla01w, bioID_barnead01w, bioID_barnequ01w, bioID_batkosu01w, bioID_batteja01w, bioID_battlas01w, bioID_bauerca01w, bioID_beardal01w, bioID_beckki01w, bioID_bennije01w, bioID_berezva01w, bioID_berthlu01w, bioID_beviltu01w, bioID_bibbyje01w, bioID_bibrzag01w, bioID_birdsu01w, bioID_bjorkte01w, bioID_blackch01w, bioID_blackde01w, bioID_blodgci01w, bioID_blueni01w, bioID_blueoc01w, bioID_bobbish01w, bioID_boddiwh01w, bioID_boltoru01w, bioID_bondla01w, bioID_bonfisu01w, bioID_bonnede01w, bioID_bowenli01w, bioID_boydca01w, bioID_braxtka01w, bioID_brazian01w, bioID_bristre01w, bioID_brogami01w, bioID_brondsa01w, bioID_brownco01w, bioID_browned01w, bioID_brownki01w, bioID_brownru01w, bioID_brumfma01w, bioID_brungje01w, bioID_brunsre01w, bioID_bullevi01w, bioID_burgean01w, bioID_burgeli01w, bioID_burraal01w, bioID_burseja01w, bioID_buttsta01w, bioID_byearla01w, bioID_campbed01w, bioID_campbmi01w, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 735 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 720)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilizador\\Documents\\M.EIC\\AC\\feup-ac\\notebook.ipynb Cell 62\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y345sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y345sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m train_inputs \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(train_inputs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Documents/M.EIC/AC/feup-ac/notebook.ipynb#Y345sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m test_inputs \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(test_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1005\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m-> 1006\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1007\u001b[0m     X,\n\u001b[0;32m   1008\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1009\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1010\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1011\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m   1012\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m   1016\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    966\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 967\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    968\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    969\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    973\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    974\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 720)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# print(df['birthDate'])\n",
    "\n",
    "print(df)\n",
    "# # get all rows from df where year = 11\n",
    "test_data = df.loc[df[\"year\"] == 11]\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "# # get all rows from df where year <> 11\n",
    "train_data = df.loc[df[\"year\"] != 11]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "# print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
