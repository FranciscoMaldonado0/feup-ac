{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "connection = sqlite3.connect(\"./database/test.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "labelEncoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1º Try - The most basic model possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first model we created, and it served as a foundation for testing various machine learning algorithms. We only chose the **team ID**, **player ID**, **year**, and **playoff** (whether the team made the playoffs or not) for this model.\n",
    "\n",
    "The data was divided into training (the first nine years of data) and test sets (the tenth year of data). Also, due to the inclusion of the player IDs, we aren’t determining which teams go to the playoffs, but which players go instead. This type of division into training and test sets was also used in the next models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select teams.tmID as tmID, playerID, teams.year as year, playoff from players_teams join teams \\\n",
    "on teams.tmID = players_teams.tmID and teams.year = players_teams.year order by teams.year, teams.name ;\"\n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "categorical_columns = [\"tmID\", \"playerID\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = labelEncoder.fit_transform(df[col].astype('str'))\n",
    "    \n",
    "# replace 'Y' and 'N' with 1 and 0 in the playoff column\n",
    "df[\"playoff\"] = df[\"playoff\"].replace(\"Y\", 1)\n",
    "df[\"playoff\"] = df[\"playoff\"].replace(\"N\", 0)\n",
    "    \n",
    "print(df.head())\n",
    "    \n",
    "# split the data into train and test\n",
    "\n",
    "train_df = df[df[\"year\"] < 10]\n",
    "\n",
    "train_inputs = train_df.iloc[:, :-1].values\n",
    "train_labels = train_df.iloc[:, -1].values\n",
    "    \n",
    "test_df = df[df[\"year\"] == 10]\n",
    "\n",
    "print(test_df)\n",
    "\n",
    "test_inputs = test_df.iloc[:, :-1].values\n",
    "test_labels = test_df.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we decided to test it with the four different ML algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train_inputs,train_labels)\n",
    "\n",
    "\n",
    "tree.plot_tree(clf)\n",
    "\n",
    "# metrics for the decision tree\n",
    "y_pred = clf.predict(test_inputs)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = clf.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "\n",
    "# metrics for the logistic regression\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = logreg.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = gnb.predict(test_inputs)\n",
    "\n",
    "# metrics for the naive bayes\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = gnb.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = svm.predict(test_inputs)\n",
    "\n",
    "# metrics for the SVM\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach all 4 algorithms performed poorly. This was expected considering the data they are trained on is very limited since it does not include important statistics and only if the set player + team went to the playoffs in a given year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2º Try - Team Performance in Consideration\n",
    "\n",
    "To improve the quality of our predictions in comparison with the previous model we decided to train our model based on the teams average statistics per player from the previous year. \n",
    "\n",
    "With this approach we solve the problem of having predictions on the players instead of on the teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dataframe without any column names, indices or data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "## obtain the team IDS and year\n",
    "team_ids = pd.read_sql(\"select tmID, year from teams\", connection)\n",
    "\n",
    "## iterate through each team and year\n",
    "for index, row in team_ids.iterrows():\n",
    "    ## obtain the team ID and year for each row\n",
    "    team_id = row['tmID']\n",
    "    year = row['year']\n",
    "    \n",
    "    if(year == 1):\n",
    "        continue\n",
    "    \n",
    "    query = \"select tmID, year + 1 as year, avg(oRebounds), avg(dRebounds), avg(rebounds), avg(assists), avg(steals), avg(blocks), \\\n",
    "        avg(turnovers), avg(PF), avg(fgAttempted), avg(fgMade), avg(ftAttempted), avg(ftMade), avg(threeAttempted), avg(threeMade) \\\n",
    "        from players_teams where tmID = '\" + team_id + \"' and year = \" + str(year - 1) + \";\"\n",
    "        \n",
    "    ## obtain the average stats for each team\n",
    "    team_stats = pd.read_sql(query, connection)\n",
    "    \n",
    "    query = \"select avg(weight), avg(height) from players_teams join players on players.bioID = players_teams.playerID \\\n",
    "        where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "        \n",
    "    ## obtain the average weight and height for each team\n",
    "    team_weight_height = pd.read_sql(query, connection)\n",
    "    \n",
    "    ## add the average weight and height to the team stats\n",
    "    team_stats['weight'] = team_weight_height['avg(weight)']\n",
    "    team_stats['height'] = team_weight_height['avg(height)']\n",
    "    \n",
    "    # transform the NaN values to 0\n",
    "    team_stats = team_stats.fillna(0)\n",
    "    \n",
    "    # include in the query the win and loss ratio\n",
    "    query = \"select tmID, year, won, lost, GP from teams where tmID = '\" + team_id + \"' and year = \" + str(year - 1) + \";\"\n",
    "    \n",
    "    team_win_loss = pd.read_sql(query, connection)\n",
    "    \n",
    "    # obtain the win and loss ratio\n",
    "    win_ratio = team_win_loss['won'] / team_win_loss['GP']\n",
    "    loss_ratio = team_win_loss['lost'] / team_win_loss['GP']\n",
    "    \n",
    "    # add the win and loss ratio to the team stats\n",
    "    team_stats['win_ratio'] = win_ratio\n",
    "    team_stats['loss_ratio'] = loss_ratio\n",
    "    \n",
    "    query = \"select count(pt.playerID) as num_player_awards from players_teams pt join awards_players ap on pt.playerID = ap.playerID \\\n",
    "        where pt.year = ap.year and pt.tmID = '\" + team_id + \"' and pt.year < \" + str(year) + \" group by pt.tmID, ap.year;\"\n",
    "        \n",
    "    team_awards = pd.read_sql(query, connection)\n",
    "    \n",
    "    # add the number of player awards to the team stats\n",
    "    team_stats['num_player_awards'] = team_awards['num_player_awards']\n",
    "    \n",
    "    # transform the NaN values to 0\n",
    "    team_stats = team_stats.fillna(0)\n",
    "    \n",
    "    # obtain the number of coach awards\n",
    "    query = \"select count(coachID) as num_coach_awards from coaches join awards_players ap on coaches.coachID = ap.playerID \\\n",
    "        where coaches.year = ap.year and coaches.tmID = '\" + team_id + \"' and coaches.year < \" + str(year) + \" group by coaches.tmID, ap.year;\"\n",
    "        \n",
    "    team_coach_awards = pd.read_sql(query, connection)\n",
    "    \n",
    "    # add the number of coach awards to the team stats\n",
    "    team_stats['num_coach_awards'] = team_coach_awards['num_coach_awards']\n",
    "    \n",
    "    # transform the NaN values to 0\n",
    "    team_stats = team_stats.fillna(0)\n",
    "        \n",
    "    query = \"select tmID, year from teams where semis <> '' and tmID = '\" + team_id + \"' and year = \" + str(year - 1) + \";\"\n",
    "    semis = pd.read_sql(query, connection)\n",
    "    \n",
    "    # team reached semis\n",
    "    if semis.empty:\n",
    "        team_stats['semis'] = 0\n",
    "    else:\n",
    "        team_stats['semis'] = 1\n",
    "        \n",
    "    query = \"select tmID, year from teams where finals <> '' and tmID = '\" + team_id + \"' and year = \" + str(year - 1) + \";\"\n",
    "    finals = pd.read_sql(query, connection)\n",
    "    \n",
    "    # team reached finals\n",
    "    if finals.empty:\n",
    "        team_stats['finals'] = 0\n",
    "    else:\n",
    "        team_stats['finals'] = 1\n",
    "        \n",
    "    query = \"select playoff from teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "    playoff = pd.read_sql(query, connection)\n",
    "    \n",
    "    # check if it's Y or N\n",
    "    if playoff['playoff'][0] == 'Y':\n",
    "        team_stats['playoff'] = 1\n",
    "    else:\n",
    "        team_stats['playoff'] = 0\n",
    "        \n",
    "    \n",
    "    # append the stats to the dataframe\n",
    "    df = df._append(team_stats, ignore_index=True)\n",
    "    \n",
    "    if(year == 10):\n",
    "        print(team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical columns\n",
    "\n",
    "team_pairs = []\n",
    "team_names = df[\"tmID\"].values\n",
    "df[\"tmID\"] = labelEncoder.fit_transform(df[\"tmID\"].astype('str'))   \n",
    "\n",
    "team_pairs = set(zip(team_names, df[\"tmID\"].values))\n",
    "\n",
    "print(team_pairs)   \n",
    "        \n",
    "# remove the rows that have year = 10\n",
    "train_df = df[df.year < 10]\n",
    "\n",
    "# remove the rows that have year <> 10\n",
    "test_df = df[df.year == 10]\n",
    "\n",
    "# create the training labels and inputs\n",
    "train_inputs = train_df.iloc[:, :-1].values\n",
    "train_labels = train_df.iloc[:, -1].values\n",
    "\n",
    "# create the testing labels and inputs\n",
    "test_inputs = test_df.iloc[:, :-1].values\n",
    "test_labels = test_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train_inputs,train_labels)\n",
    "\n",
    "\n",
    "tree.plot_tree(clf)\n",
    "\n",
    "# metrics for the decision tree\n",
    "y_pred = clf.predict(test_inputs)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = clf.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = clf.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = test_df[\"tmID\"].values[i]\n",
    "    \n",
    "    # obtain the team name from the team_pairs\n",
    "    team_name = [x[0] for x in team_pairs if x[1] == team_id]\n",
    "    \n",
    "    if team_name in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_name[0], probs[i][1]))\n",
    "        \n",
    "    if team_name in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_name[0], probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=2000)\n",
    "logreg.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "\n",
    "# metrics for the logistic regression\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = logreg.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = test_df[\"tmID\"].values[i]\n",
    "    \n",
    "    # obtain the team name from the team_pairs\n",
    "    team_name = [x[0] for x in team_pairs if x[1] == team_id]\n",
    "    \n",
    "    if team_name in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_name[0], probs[i][1]))\n",
    "        \n",
    "    if team_name in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_name[0], probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = gnb.predict(test_inputs)\n",
    "\n",
    "# metrics for the naive bayes\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "y_pred_proba = gnb.predict_proba(test_inputs)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(test_labels, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = gnb.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = test_df[\"tmID\"].values[i]\n",
    "    \n",
    "    # obtain the team name from the team_pairs\n",
    "    team_name = [x[0] for x in team_pairs if x[1] == team_id]\n",
    "    \n",
    "    if team_name in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_name[0], probs[i][1]))\n",
    "        \n",
    "    if team_name in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_name[0], probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(train_inputs, train_labels)\n",
    "\n",
    "y_pred = svm.predict(test_inputs)\n",
    "\n",
    "# metrics for the SVM\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision = metrics.precision_score(test_labels, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "f1 = metrics.f1_score(test_labels, y_pred)\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = svm.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = test_df[\"tmID\"].values[i]\n",
    "    \n",
    "    # obtain the team name from the team_pairs\n",
    "    team_name = [x[0] for x in team_pairs if x[1] == team_id]\n",
    "    \n",
    "    if team_name in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_name[0], probs[i][1]))\n",
    "        \n",
    "    if team_name in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_name[0], probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach we got better results than in the previous one but they are still far from good.\n",
    "\n",
    "SVM, Logistic Regression and Naive Bayes were able to produce 6 True Positive classifications.\n",
    "\n",
    "However it still has some limitations, namely the fact that it can't deal with a change in the teams composition and it also does not use information regarding coaches and awards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd try - Based on the players perfomance\n",
    "\n",
    "In a third approach we decided to try to predict the player performance considering their historical data and then consider the teams as a sum of all players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from players join players_teams on players.bioID = players_teams.playerID;\", connection)\n",
    "\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stint', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# get bioID and year from the dataframe\n",
    "bioID = df[\"bioID\"].values\n",
    "year = df[\"year\"].values\n",
    "\n",
    "iterable = zip(bioID, year)\n",
    "\n",
    "# iterate through the (bioID, year) pairs\n",
    "\n",
    "for bioID, year in iterable:\n",
    "        # get number of awards for the player in the team in the year\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + bioID + \"' and ap.year <= \" + str(year) + \";\"\n",
    "                \n",
    "        player_awards = pd.read_sql(query, connection)\n",
    "        \n",
    "        # if(player_awards[\"num_awards_player\"].values[0] > 0):\n",
    "        #         print(bioID, year, player_awards)\n",
    "                \n",
    "        # add number of awards to the dataframe\n",
    "        df.loc[(df[\"bioID\"] == bioID) & (df[\"year\"] == year), \"num_awards_player\"] = player_awards[\"num_awards_player\"].values[0]\n",
    "     \n",
    "# extract year from birthDate\n",
    "df[\"birthDate\"] = pd.to_datetime(df[\"birthDate\"])\n",
    "df[\"birthDate\"] = df[\"birthDate\"].dt.year\n",
    "\n",
    "player_ids_10 = df[df[\"year\"] == 10][\"bioID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical data\n",
    "categorical_columns = ['bioID', 'pos', 'college', 'collegeOther', 'tmID']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "print(df['birthDate'])\n",
    "\n",
    "# # get all rows from df where year = 10\n",
    "test_data = df.loc[df[\"year\"] == 10]\n",
    "\n",
    "# # get all rows from df where year <> 10\n",
    "train_data = df.loc[df[\"year\"] < 10]\n",
    "\n",
    "\n",
    "labels = ['points', 'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals',\n",
    "       'blocks', 'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted',\n",
    "       'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n",
    "\n",
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_inputs = scaler.fit_transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predicted each player statistic using a Neural Network and, considering there are multiple target variables, we extended the MLPRegressor with a MultiOutputRegressor to predict all the variables needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100, 100, 100), \n",
    "    max_iter=2000,batch_size=32, alpha=0.0001, solver='adam', verbose=10, random_state=21, tol=0.000000001))\n",
    "model.fit(train_inputs, train_labels)\n",
    "\n",
    "# test model\n",
    "predictions = model.predict(test_inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all predictions to integers\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        predictions[i][j] = int(round(predictions[i][j]))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    # print(\"player: \", i)\n",
    "    print(\"player: \", player_ids_10[i])\n",
    "    for j in range(len(predictions[i])):\n",
    "        print(labels[j], \":  predicted: \", predictions[i][j], \" actual: \", test_labels[i][j])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to group all players by teams and sum their statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get team ids from the dataframe from all years \n",
    "team_ids = set(df[\"tmID\"].values)\n",
    "\n",
    "# create a dictionary where the keys are the team ids and the values are the avg stats for the team\n",
    "avg_team_stats = {}\n",
    "\n",
    "for team_id in team_ids:\n",
    "    avg_team_stats[team_id] = [0] * len(labels)\n",
    "    \n",
    "print(test_data.columns)\n",
    "\n",
    "# iterate through the team ids\n",
    "for team_id in team_ids:\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    " \n",
    "        if(test_data[\"tmID_\" + team_id].values[i] == 1):\n",
    "            avg_team_stats[team_id] = [x + y for x, y in zip(avg_team_stats[team_id], predictions[i])]\n",
    "            \n",
    "        \n",
    "# remove tmID from the dictionary that contain all the avg stats for the teams at 0\n",
    "for team_id in team_ids:\n",
    "    if(avg_team_stats[team_id] == [0] * len(labels)):\n",
    "        del avg_team_stats[team_id]\n",
    "        \n",
    "        \n",
    "print(avg_team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from teams where year < 10;\"\n",
    "\n",
    "team_stats = pd.read_sql(query, connection)\n",
    "\n",
    "columns = ['tmID', 'year','o_pts', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_stl',\n",
    "       'o_blk', 'o_to', 'o_pf', 'o_fga', 'o_fgm', 'o_fta',\n",
    "       'o_ftm', 'o_3pa', 'o_3pm', 'confID', 'playoff']\n",
    "\n",
    "team_stats = team_stats[columns]\n",
    "\n",
    "query = \"select tmID from teams where year = 10;\"\n",
    "team_ids = pd.read_sql(query, connection)\n",
    "team_ids = team_ids[\"tmID\"].values\n",
    "\n",
    "team_stats = team_stats.drop(columns=[\"tmID\"])\n",
    "\n",
    "categorical_columns = ['confID']\n",
    "\n",
    "for col in categorical_columns:\n",
    "       team_stats[col] = team_stats[col].astype('category')\n",
    "       \n",
    "team_stats = pd.get_dummies(team_stats, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)\n",
    "\n",
    "print(train_inputs)\n",
    "print(test_inputs)\n",
    "\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "query = \"select tmId, confID from teams where year = 10;\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"select tmID from teams where year = 10;\"\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, alpha=0.0001, solver='adam', verbose=0, random_state=21, tol=0.000000001)\n",
    "mlp.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = mlp.predict(test_inputs)\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = mlp.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "clf.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = clf.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = clf.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=24)\n",
    "knn.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = knn.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = knn.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(train_inputs,train_labels)\n",
    "\n",
    "#print decision tree\n",
    "tree.plot_tree(dt)\n",
    "columns = ['bioID', 'pos', 'height', 'weight', 'college', 'collegeOther',\n",
    "       'birthDate', 'year', 'stint', 'tmID', 'points', 'oRebounds', 'dRebounds', 'rebounds',\n",
    "       'assists', 'steals', 'blocks', 'turnovers', 'PF', 'fgAttempted',\n",
    "       'fgMade', 'ftAttempted', 'ftMade', 'threeAttempted', 'threeMade']\n",
    "\n",
    "y_pred = dt.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred, pos_label=\"Y\"))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = dt.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "        \n",
    "        team_id = team_ids[i]\n",
    "        \n",
    "        if team_id in west_teams[\"tmID\"].values:\n",
    "            probs_west.append((team_id, probs[i][1]))\n",
    "            \n",
    "        if team_id in east_teams[\"tmID\"].values:\n",
    "            probs_east.append((team_id, probs[i][1]))\n",
    "        \n",
    "        # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "        \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th try - The best model\n",
    "\n",
    "Considering the data context we decided to develop a more complex model that considers teams as a set of players and coach.\n",
    "Our goal is to make the final prediction based in the expected players performance according to their recorded statistics in the previous years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dataframe without any column names, indices or data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "## obtain the team IDS and year\n",
    "team_ids = pd.read_sql(\"select tmID, year, confID from teams order by tmID\", connection)\n",
    "\n",
    "## iterate through each team and year\n",
    "for index, row in team_ids.iterrows():\n",
    "\n",
    "    ## obtain the team ID and year for each row\n",
    "    team_id = row['tmID']\n",
    "    year = row['year']\n",
    "    confID = row['confID']\n",
    "\n",
    "    # get all players from the team and year\n",
    "    query = \"select tmID, year, playerID from players_teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "\n",
    "    ## obtain the players for each team\n",
    "    team_players = pd.read_sql(query, connection)\n",
    "    # points, oRebounds, dRebounds, rebounds, assists, steals, blocks, turnovers, PF, fgAttempted, fgMade, ftAttempted, ftMade, threeAttempted, threeMade\n",
    "    team_stats = {\"year\": year, \"points\": 0, \"oRebounds\": 0, \"dRebounds\": 0, \"rebounds\": 0,\n",
    "                  \"assists\": 0, \"steals\": 0, \"blocks\": 0, \"turnovers\": 0, \"PF\": 0, \"fgAttempted\": 0,\n",
    "                  \"fgMade\": 0, \"ftAttempted\": 0, \"ftMade\": 0, \"threeAttempted\": 0, \"threeMade\": 0,\n",
    "                  'weight': 0, \"height\": 0, \"player_awards\": 0, \"confID\": confID, \"num_playoffs\": 0,\n",
    "                  \"num_semis\": 0, \"num_finals\": 0, \"coach_win_ratio\": 0, \"coach_stint\": 0,\n",
    "                  \"playoff\": 0}\n",
    "\n",
    "    # iterate through each player\n",
    "    for idx, r in team_players.iterrows():\n",
    "        ## obtain the player ID for each row\n",
    "        player_id = r['playerID']\n",
    "\n",
    "        # get the player's position\n",
    "        query = \"select pos from players where bioID = '\" + player_id + \"';\"\n",
    "        pos = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        # get number of years played\n",
    "        query = \"select count(distinct year) as num_years from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        num_years = pd.read_sql(query, connection).values[0][0]\n",
    "\n",
    "        if num_years == 0:\n",
    "            query = \"select avg(points) as points, avg(oRebounds) as oRebounds, avg(dRebounds) as dRebounds, avg(rebounds) as rebounds, avg(assists) as assists, \\\n",
    "                avg(steals) as steals, avg(blocks) as blocks, avg(turnovers) as turnovers, avg(PF) as PF, avg(fgAttempted) as fgAttempted, \\\n",
    "                avg(fgMade) as fgMade, avg(ftAttempted) as ftAttempted, avg(ftMade) as ftMade, avg(threeAttempted) as threeAttempted, avg(threeMade) as threeMade \\\n",
    "                from players_teams join players on players_teams.playerID = players.bioID where year = \" + str(year - 1) + \" \\\n",
    "                and pos = '\" + pos + \"' and playerID not in (select playerID from players_teams where year < \" + str(year - 1) + \");\"\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            ## add the player stats to the team stats\n",
    "            team_stats['points'] += (player_stats['points'].values[0] or 0 )\n",
    "            team_stats['oRebounds'] += (player_stats['oRebounds'].values[0] or 0 )\n",
    "            team_stats['dRebounds'] += (player_stats['dRebounds'].values[0] or 0 )\n",
    "            team_stats['rebounds'] += (player_stats['rebounds'].values[0] or 0 )\n",
    "            team_stats['assists'] += (player_stats['assists'].values[0] or 0 )\n",
    "            team_stats['steals'] += (player_stats['steals'].values[0] or 0 )\n",
    "            team_stats['blocks'] += (player_stats['blocks'].values[0] or 0 )\n",
    "            team_stats['turnovers'] += (player_stats['turnovers'].values[0] or 0 )\n",
    "            team_stats['PF'] += (player_stats['PF'].values[0] or 0 )\n",
    "            team_stats['fgAttempted'] += (player_stats['fgAttempted'].values[0] or 0 )\n",
    "            team_stats['fgMade'] += (player_stats['fgMade'].values[0] or 0 )\n",
    "            team_stats['ftAttempted'] += (player_stats['ftAttempted'].values[0] or 0 )\n",
    "            team_stats['ftMade'] += (player_stats['ftMade'].values[0] or 0 )\n",
    "            team_stats['threeAttempted'] += (player_stats['threeAttempted'].values[0] or 0 )\n",
    "            team_stats['threeMade'] += (player_stats['threeMade'].values[0] or 0 )\n",
    "            \n",
    "        else:\n",
    "\n",
    "            query = \"select year, points, oRebounds, dRebounds, rebounds, assists, \\\n",
    "                    steals, blocks, turnovers, PF, fgAttempted, \\\n",
    "                    fgMade, ftAttempted, ftMade, threeAttempted, sum(threeMade) as threeMade \\\n",
    "                    from players_teams where playerID = '\" + player_id + \"' and year < \" + str(year) + \";\"\n",
    "        \n",
    "            ## obtain the average stats for each team\n",
    "            player_stats = pd.read_sql(query, connection)\n",
    "\n",
    "            points = oRebounds = dRebounds = rebounds = assists = steals = blocks = turnovers = PF = fgAttempted = fgMade = ftAttempted = ftMade = threeAttempted = threeMade = 0\n",
    "            sum_weight = 0\n",
    "            # iterate player stats and make weighted average for each year\n",
    "            for j, row2 in player_stats.iterrows():\n",
    "\n",
    "                iteration_year = row2['year']\n",
    "\n",
    "                weight = 1 / (year - iteration_year)\n",
    "\n",
    "                sum_weight += weight\n",
    "\n",
    "                points += row2['points'] * weight\n",
    "                oRebounds += row2['oRebounds'] * weight\n",
    "                dRebounds += row2['dRebounds'] * weight\n",
    "                rebounds += row2['rebounds'] * weight\n",
    "                assists += row2['assists'] * weight\n",
    "                steals += row2['steals'] * weight\n",
    "                blocks += row2['blocks'] * weight\n",
    "                turnovers += row2['turnovers'] * weight\n",
    "                PF += row2['PF'] * weight\n",
    "                fgAttempted += row2['fgAttempted'] * weight\n",
    "                fgMade += row2['fgMade'] * weight\n",
    "                ftAttempted += row2['ftAttempted'] * weight\n",
    "                ftMade += row2['ftMade'] * weight\n",
    "                threeAttempted += row2['threeAttempted'] * weight\n",
    "                threeMade += row2['threeMade'] * weight\n",
    "\n",
    "            # add the player stats to the team stats\n",
    "            team_stats['points'] += points / sum_weight\n",
    "            team_stats['oRebounds'] += oRebounds / sum_weight\n",
    "            team_stats['dRebounds'] += dRebounds / sum_weight\n",
    "            team_stats['rebounds'] += rebounds / sum_weight\n",
    "            team_stats['assists'] += assists / sum_weight\n",
    "            team_stats['steals'] += steals / sum_weight\n",
    "            team_stats['blocks'] += blocks / sum_weight\n",
    "            team_stats['turnovers'] += turnovers / sum_weight\n",
    "            team_stats['PF'] += PF / sum_weight\n",
    "            team_stats['fgAttempted'] += fgAttempted / sum_weight\n",
    "            team_stats['fgMade'] += fgMade / sum_weight\n",
    "            team_stats['ftAttempted'] += ftAttempted / sum_weight\n",
    "            team_stats['ftMade'] += ftMade / sum_weight\n",
    "            team_stats['threeAttempted'] += threeAttempted / sum_weight\n",
    "            team_stats['threeMade'] += threeMade / sum_weight\n",
    "        \n",
    "        num_years = max(num_years, 1)\n",
    "\n",
    "\n",
    "        # get each player num awards\n",
    "        query = \"select count(award) as num_awards_player from awards_players ap join players_teams pt on ap.year = pt.year \\\n",
    "                and ap.playerID = pt.playerID where ap.playerID = '\" + player_id + \"' and ap.year < \" + str(year) + \";\"\n",
    "        num_awards_player = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['player_awards'] += num_awards_player\n",
    "\n",
    "        # get the number of times the player went to the playoffs\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and firstRound <> '';\"\n",
    "        num_playoffs = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_playoffs'] += num_playoffs\n",
    "\n",
    "        # get the number of times the player went to the semis\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and semis <> '';\"\n",
    "        num_semis = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_semis'] += num_semis\n",
    "\n",
    "        # get the number of times the player went to the finals\n",
    "        query = \"select count(*) as num_playoffs from teams join players_teams on teams.year = players_teams.year and teams.tmID = players_teams.tmID\\\n",
    "            where playerID = '\" + player_id + \"' and teams.year < \" + str(year) + \" and finals <> '';\"\n",
    "        num_finals = pd.read_sql(query, connection).values[0][0]\n",
    "        team_stats['num_finals'] += num_finals\n",
    "    \n",
    "    \n",
    "    query = \"select avg(weight), avg(height) from players_teams join players on players.bioID = players_teams.playerID \\\n",
    "        where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "        \n",
    "    ## obtain the average weight and height for each team\n",
    "    team_weight_height = pd.read_sql(query, connection)\n",
    "    \n",
    "    ## add the average weight and height to the team stats\n",
    "    team_stats['weight'] = team_weight_height['avg(weight)'].values[0]\n",
    "    team_stats['height'] = team_weight_height['avg(height)'].values[0]\n",
    "\n",
    "    # get team coach\n",
    "    query = \"select coachID, stint from coaches where tmID = '\" + team_id + \"' and year = \"+ str(year) +\";\"\n",
    "    coach_id = pd.read_sql(query, connection).values[0][0]\n",
    "    stint = pd.read_sql(query, connection).values[0][1]\n",
    "    team_stats['coach_stint'] = stint\n",
    "    \n",
    "    \n",
    "    query = \"select count(award) as num_awards from coaches join awards_players \\\n",
    "              on coaches.coachID = awards_players.playerID and coaches.year = awards_players.year \\\n",
    "              where coachID = '\" + coach_id + \"' and coaches.year < \" + str(year) + \";\"\n",
    "    coach_awards = pd.read_sql(query, connection).values[0][0]\n",
    "    team_stats['coach_awards'] = coach_awards\n",
    "\n",
    "    # get average wins and losses from coach\n",
    "    query = \"select avg(won), avg(lost) from coaches where coachID = '\" + coach_id + \"' and year < \" + str(year) + \";\"\n",
    "    coach_wins_losses = pd.read_sql(query, connection)\n",
    "    num_matches_coach = (coach_wins_losses['avg(won)'].values[0] or 0) + (coach_wins_losses['avg(lost)'].values[0] or 0)\n",
    "    if num_matches_coach == 0:\n",
    "        team_stats['coach_win_ratio'] = 0\n",
    "    else:\n",
    "        team_stats['coach_win_ratio'] = (coach_wins_losses['avg(won)'].values[0] or 0) / num_matches_coach\n",
    "        \n",
    "        \n",
    "    query = \"select playoff from teams where tmID = '\" + team_id + \"' and year = \" + str(year) + \";\"\n",
    "    playoff = pd.read_sql(query, connection)\n",
    "    \n",
    "    # check if it's Y or N\n",
    "    if playoff['playoff'][0] == 'Y':\n",
    "        team_stats['playoff'] = 1\n",
    "    else:\n",
    "        team_stats['playoff'] = 0\n",
    "        \n",
    "    \n",
    "    # append the stats to the dataframe\n",
    "    df = df._append(team_stats, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a entry for each team for each year. For each of this entries a team is represented by:\n",
    " - the sum of their players yearly statistics weighted average \n",
    " - the average weight and height\n",
    " - the sum of players awards\n",
    " - the sum of the number of times the players reached playoffs, semis and finals\n",
    " - coach win ratio\n",
    " - conference the team belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"confID\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "df= pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we need to split the data into training and testing sets. According to the context the division that makes sense is by years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_year = 10\n",
    "\n",
    "# # get all rows from df where year = target year\n",
    "test_data = df.loc[df[\"year\"] == target_year]\n",
    "\n",
    "# # get all rows from df where year <> target_year and year <> 1\n",
    "train_data = df.loc[df[\"year\"] < target_year]\n",
    "train_data = train_data.loc[train_data[\"year\"] != 1]\n",
    "\n",
    "labels = ['playoff']\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in labels:\n",
    "        inputs.append(col)\n",
    "\n",
    "train_inputs = train_data[inputs].values\n",
    "train_labels = train_data[labels].values\n",
    "\n",
    "test_inputs = test_data[inputs].values\n",
    "test_labels = test_data[labels].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the features are in different scales it is a good practice to standardize it into a range from 0 to 1. This can be achieved using the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set we can start applying the algorithms.\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "logreg.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = logreg.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model testing phase it is useful to gather some metrics relative to the way it is working and the quality of its predictions.\n",
    "\n",
    "With this in mind we considered relevant metrics:\n",
    " - Accuracy\n",
    " - Precision\n",
    " - Recall\n",
    " - F1\n",
    " - Confusion Matrix\n",
    " - ROC\n",
    " - AUC\n",
    "\n",
    "(Note: this will only work when testing the model, so the target year variable should be set to 10. When the target year is 11 then there will be no test labels to compare with the predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "feature_importance = abs(logreg.coef_[0])\n",
    "feature_importance = sorted(zip(inputs, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print(\"Feature importance:\")\n",
    "# for i in range(len(feature_importance)):\n",
    "#     print(f\"{feature_importance[i][0]}: {feature_importance[i][1]}\")\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(test_labels, logreg.predict(test_inputs))\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, logreg.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the algorithm does not know that only 4 teams from each conference qualify to the playoffs we need to enforce this knowledge in our predictions. So, we divide the teams by conference, and consider that they qualify if they are one of the 4 most likely to, even if the probability is below 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select tmId, confID from teams where year = \" + str(target_year) + \";\"\n",
    "teams_conf_ids = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "\n",
    "# print(teams_conf_ids)\n",
    "\n",
    "west_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"WE\"]\n",
    "east_teams = teams_conf_ids[teams_conf_ids[\"confID\"] == \"EA\"]\n",
    "\n",
    "# print(west_teams)\n",
    "# print(east_teams)\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = logreg.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "query = \"select tmID from teams where year = \" + str(target_year) + \";\"\n",
    "team_ids = pd.read_sql(query, connection)\n",
    "team_ids = team_ids[\"tmID\"].values\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "\n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have the logistic regression predictions.\n",
    "\n",
    "Now we can train and predict using different algorithms.\n",
    "\n",
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=2000, alpha=0.0001, solver='adam', verbose=0, random_state=21, tol=0.000000001)\n",
    "mlp.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = mlp.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "neural_net_roc_auc = roc_auc_score(test_labels, mlp.predict(test_inputs))\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, mlp.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Neural Network (area = %0.2f)' % neural_net_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the probabilities for each class\n",
    "probs = mlp.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "knn.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = knn.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "knn_roc_auc = roc_auc_score(test_labels, knn.predict(test_inputs))  \n",
    "fpr, tpr, thresholds = roc_curve(test_labels, knn.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='KNN (area = %0.2f)' % knn_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = knn.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    \n",
    "    team_id = team_ids[i]\n",
    "    \n",
    "    if team_id in west_teams[\"tmID\"].values:\n",
    "        probs_west.append((team_id, probs[i][1]))\n",
    "        \n",
    "    if team_id in east_teams[\"tmID\"].values:\n",
    "        probs_east.append((team_id, probs[i][1]))\n",
    "    \n",
    "    # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "    \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "clf.fit(train_inputs,train_labels)\n",
    "\n",
    "y_pred = clf.predict(test_inputs)\n",
    "\n",
    "#print metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels,y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "print(\"F1:\",metrics.f1_score(test_labels, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "sb.heatmap(confusion, annot=True, fmt=\"g\", linewidths=.5, square = True, xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "all_sample_title = 'Passed to the playoffs?'\n",
    "\n",
    "plt.title(all_sample_title, size = 10)\n",
    "plt.show()\n",
    "\n",
    "# roc curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "svm_roc_auc = roc_auc_score(test_labels, clf.predict(test_inputs))\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, clf.predict_proba(test_inputs)[:,1])\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='SVM (area = %0.2f)' % svm_roc_auc)\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "# print the probabilities for each class\n",
    "probs = clf.predict_proba(test_inputs)\n",
    "probs_west = []\n",
    "probs_east = []\n",
    "\n",
    "for i in range(len(probs)):\n",
    "        \n",
    "        team_id = team_ids[i]\n",
    "        \n",
    "        if team_id in west_teams[\"tmID\"].values:\n",
    "            probs_west.append((team_id, probs[i][1]))\n",
    "            \n",
    "        if team_id in east_teams[\"tmID\"].values:\n",
    "            probs_east.append((team_id, probs[i][1]))\n",
    "        \n",
    "        # print(f\"{team_name[0]}: {probs[i][1]}\")\n",
    "        \n",
    "probs_west = sorted(probs_west, key=lambda x: x[1], reverse=True)\n",
    "probs_east = sorted(probs_east, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(probs_west)\n",
    "print(probs_east)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
